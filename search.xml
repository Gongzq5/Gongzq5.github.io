<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Springboot+Mybatis基本姿势]]></title>
    <url>%2Fposts%2FSpringboot-Mybatis%E5%9F%BA%E6%9C%AC%E5%A7%BF%E5%8A%BF-2021-03-23%2F</url>
    <content type="text"><![CDATA[Springboot + Mybatis 基本姿势最近做了个CS5224 Cloud Computing的课程项目，由于前端打算做个Android App，后端为了统一也就选用了Java开发。 项目团队的地址在 https://github.com/nus-cs5224-team/； 极速入门了一波 Springboot，Mybatis，复习了一波 SQL，感觉还是开发比较快乐； 基本框架目录结构123456789101112131415161718192021222324252627main ├─java │ └─edu │ └─nus │ └─campus │ │ CampusApplication.java │ ├─config │ │ SwaggerConfig.java │ ├─controller │ │ BuildingController.java │ │ ... │ ├─mappers │ │ BuildingMapper.java │ │ ... │ └─model │ Building.java │ ... └─resources │ application.properties ├─mybatis │ │ mybatis-config.xml │ └─mappers │ BuildingMapper.xml │ ... └─sql fake_data.sql initial.sql 从上到下说， CampusApplication.java 是项目文件的主入口； config文件夹里放的是Swagger的配置，不用Swagger生成文档的话就不需要这个文件夹了； controller是路由，通过Spring提供的注解，接受http请求，进行数据库查询和一些逻辑操作，然后返回结果； mappers是mybatis的dao层，这里写的都是interface类型，mybatis会自动从resources/mybatis/mappers/*.xml描述的SQL里自动生成方法填充这个类； model是我们的实体类，比如 User，比如Building，一般和数据库里的table对应，可以从数据库table的一行内容生成一个JavaBean实例； 然后到了 resources，主要就是 mybatis/mappers里描述的SQL，Mybatis可以自动生成一些代码； 然后放了几个SQL脚本，用来生成table，以及写一些假数据填充； 我一般的主要困惑就是项目结构如何组织，因为具体的操作方式在官方文档里通常讲的都比较详细，所以把这一节放在了第一部分，首先要搞清楚什么东西该写在哪里~ SpringbootAPI 接口用@RestController注解，非常方便可以映射到接口上，函数返回的结果可以自动转换成Json； 123456789101112@RestController@RequestMapping("/api/v1/bus")@Api(tags = "Bus API")public class BusController &#123; @ApiOperation("Get a bus by name") @GetMapping("/&#123;busName&#125;") public Bus getBus(@PathVariable("busName") String busName) &#123; // CRUD from db, get the bus return ... &#125; ...&#125; 大概就是这样，上边的那个 @Api 注解是Swagger的一部分，可以用来自动生成API文档； 其他的@PostMapping之类的，直接看文档就好了，一找一大把； 这其实就是主要用到的Springboot的功能了； Swagger(springfox)可以自动生成文档，非常好用； 12345678910111213141516@RestController@RequestMapping("/api/v1/bus")@Api(tags = "Bus API")public class BusController &#123; @Autowired private BusMapper busMapper; @ApiOperation("Get a bus by name") @GetMapping("/&#123;busName&#125;") public Bus getBus(@PathVariable("busName") String busName) &#123; // CRUD from db, get the bus if (busName == null) return null; return busMapper.findByName(busName); &#125; ...&#125; 其中的 @Api 注解，生成效果就是将以下的所有api归为一个类别显示在文档里； 其中的 @ApiOperation(&quot;Get a bus by name&quot;)注解，就是表示具体某个api的作用； swagger可以自动提取每个函数的参数，并将其转换成http请求的语言，比如上边的@PathVariable，或者用@RequestParam(, require=true|false)，也可以体现到最终的文档里； MybatisMybatis本质上就是个用xml文档来写sql的工具，可以将xml文档翻译成一个Java类，这个类提供了xml里定义的增删改查等等等等功能； Mybatis强大的地方在于他提供了一系列类似于resultMap的元素，可以将sql的结果按照一定规则映射到resultMap，然后映射到一个Java对象上； resultMap的问题在于，每个map都要对应一段java代码定义的实体类，会导致代码部分比较臃肿，所以对于我的小项目，我宁可在函数里跑两次SQL查询，而不是为每类查询都定义一个resultMap；这个主要还是看需求的，如果需求上去了，能一个SQL跑完的是一定不运行2次SQL的，毕竟性能第一嘛； xml 实例123456789101112131415&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="edu.nus.campus.mappers.StopMapper"&gt; &lt;select id="findById" resultType="edu.nus.campus.model.Stop"&gt; SELECT * FROM `stop` WHERE `id`=#&#123;id&#125;; &lt;/select&gt; &lt;select id="findRunningThroughBus" parameterType="edu.nus.campus.model.Stop" resultType="edu.nus.campus.model.Bus"&gt; SELECT * FROM `bus` WHERE `id` in (SELECT `bus_id` FROM `running_through` WHERE `stop_id`=#&#123;id&#125;); &lt;/select&gt;&lt;/mapper&gt; 第一句的namespace，表示这个Mapper会实现到一个叫StopMapper的类，其中类的方法就是xml里元素的id定义的名字； 可以看到可以用&lt;select&gt;元素表示一个select语句，返回结果可以定义为一个JavaBean类，会自动生成这样的一个实体对象。 具体的操作还有&lt;insert&gt;等等，resultType可以替换成resultMap，然后使用associate等属性提供连接的查询，会更优雅。 对应的 Mapper interface1234public interface StopMapper &#123; Stop findById(int id); List&lt;Bus&gt; findRunningThroughBus(Stop stop);&#125; 可以看到返回值的类型，直接就是Java类型了； Config配置在application.properties文件里； 12mybatis.config-location=classpath:mybatis/mybatis-config.xmlmybatis.mapper-locations=classpath:mybatis/mappers/*.xml 表示，使用/resource/mybatis/mybatis-config.xml代表的mybatis配置，并映射所有位于/resource/mybatis/mappers/*.xml的mapper。 这里的配置有很多，可以选择缓存的时效等等，我的项目没有特别的需求，也就没有调整这里的内容；]]></content>
      <categories>
        <category>后端</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>Java</tag>
        <tag>入门系列</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fposts%2FLinearRegression%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0-2021-03-09%2F</url>
    <content type="text"><![CDATA[Linear RegressionAssume the model is: \hat{y} = f(x) = X\betaWe can write a base class using this formula for our linear regression implement below, 123456789101112class LinearRegressionBase: def __init__(self): # beta: shape (p, 1) self.beta = None def fit(self, X, y): # X: shape (n, p), Y: shape (n, 1) pass def predict(self, X): # X: shape (n, p) return X @ self.beta And we can generate some data for test, 12345678n = 100p = 5noise = np.random.randn(n) / 10X = np.random.randn(n, p) * 10weights = np.ones(p)weights[[1,2,3]]=[2,5,3]y = (X @ weights + noise).reshape(-1, 1)X.shape, y.shape, X.mean(), y.mean() The code above represents the ground truth is y = 1\cdot[x]_0 + 2\cdot[x]_1 + 5\cdot[x]_2 + 3\cdot[x]_3 + 1\cdot[x]_4Ordinary Explicit SolutionUsing Mean Square Error as the loss function \begin{align*} loss &= \frac{1}{N} ||\hat{y} - y||_2^2 \\ &= \frac{1}{N} ||X\beta - y||_2^2 \end{align*}Find the derivative of $\beta$. The least square solution would be like \hat\beta = (X^T X)^{-1} X^T y \\ (\text{assume }(X^T X)\text{ invertible})Using python to implement that, 1234class LinearRegression: def fit(self, X, y): super().fit(X, y) self.beta = (np.linalg.inv(X.T @ X) @ X.T) @ y And test it, 12345678lr = LinearRegressionO()lr.fit(X, y)pred = lr.predict(X)print(f'beta &#123;lr.beta.flatten()&#125;')print(f'error &#123;np.linalg.norm(pred-y)&#125;')# beta [0.99903924 1.99955203 5.00101573 3.00037858 1.00036293]# error 0.9101951355773127 Ridge RegressionSame as last part. But we add a l2 regularization to the loss function. loss = \frac{1}{N}||X\beta-y||^2_2+\lambda||\beta||_2^2Very similar to the last part, take the derivative of $\beta$, we get \frac{\partial loss}{\partial\beta}=\frac{1}{N}(2X^TX\beta-2X^Ty)+2\lambda\betaLet the derivative to be 0. We get, \hat\beta=(X^TX+\lambda I)X^TyWe don’t need to assume the invertibility since the $(X^TX+\lambda I)$ is always invertible. Simple proof: Any vector is the eigenvector of $\lambda I$ since $\lambda I v = \lambda v$. And all eigenvalues of $\lambda I$ is $\lambda$. $X^TX$ is a symmetric matrix and it’s positive semi-definite, so all the eigenvalues are bigger or equal to 0. So any eigenvalues and eigenvectors of $X^TX$, note by $a_i, v_i$, (X^TX+\lambda I) v_i= (X^TXv_i+\lambda Iv_i)=a_iv_i+\lambda v_i=(a_i+\lambda)v_iSo all the eigenvalues of $(X^TX+\lambda I)$ is bigger or equal to $\lambda&gt;0$. Which means $(X^TX+\lambda I)$ is always invertible. Using python to implement that, 12345class RidgeRegression(LinearRegressionBase): def fit(self, X, y, l): # l is the \lambda in the formula super().fit(X, y) self.beta = (np.linalg.inv(X.T @ X + l * np.identity(X.shape[1])) @ X.T) @ y And test it, using different $\lambda$, 1234567891011l = 10:beta [0.99547771 1.99660303 4.99462785 2.99652915 0.99752248]error 1.1770827499055974l = 1beta [0.99868234 1.99925674 5.00037611 2.99999319 1.00007827]error 0.9132585574988228l = 0.1beta [0.99900354 1.9995225 5.00095176 3.00034004 1.00033446]error 0.9102258292688261 Lasso]]></content>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 11. Container With Most Water]]></title>
    <url>%2Fposts%2FLeetCode-11-Container-With-Most-Water-2020-02-14%2F</url>
    <content type="text"><![CDATA[题目链接给到 https://leetcode-cn.com/problems/container-with-most-water/ 给定 n 个非负整数$a_1, a_2, … ,a_n$，每个数代表坐标中的一个点$(i, a_i)$ 。在坐标内画 n 条垂直线，垂直线 $i$ 的两个端点分别为 $(i, ai)$ 和 $(i, 0)$。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 说明：你不能倾斜容器，且 n 的值至少为 2。 图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例: 12输入: [1,8,6,2,5,4,8,3,7]输出: 49 分析正确的方法是，开始时将两个线放在两边，然后每次都将较短的那根线向内移动，同时更新最大面积； 官方说明是这样的：[https://leetcode-cn.com/problems/container-with-most-water/solution/sheng-zui-duo-shui-de-rong-qi-by-leetcode/][https://leetcode-cn.com/problems/container-with-most-water/solution/sheng-zui-duo-shui-de-rong-qi-by-leetcode/] 证明我们可以简单的证明这个解法的正确性： 简单说明一下，$i, j$ 即左右两个端点。 最后我们可以确定，原来的面积为$3h_0$： 如果$h_x &gt; h_0$，那么面积为 $xh_0$，明显是小于 $3h_0$的； 而如果 $h_x&lt;h_0$ 的话，一方面$x&lt;3$，另一方面 $h_x&lt;h_0$，所以肯定也是小于 $3h_0$的； 我只证明了，当 $i$ 按照我们的方式移动时，不会错过任何的情况； 很明显，$j$ 也是一样的道理，所以我们的解法是不会错过最优解的。 题解12345678910111213class Solution: def maxArea(self, height: List[int]) -&gt; int: themax = 0 i1, i2 = 0, len(height) - 1 while i1 &lt; i2: area = (i2 - i1) * min(height[i2], height[i1]) if area &gt; themax: themax = area if height[i1] &lt; height[i2]: i1 += 1 else: i2 -= 1 return themax]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[科学上网姿势汇总]]></title>
    <url>%2Fposts%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E5%A7%BF%E5%8A%BF%E6%B1%87%E6%80%BB-2019-12-20%2F</url>
    <content type="text"><![CDATA[汇总一下各种姿势，不然每次部署梯子都要单独找教程；三个： ShadowSocks, SSR, v2ray SS基本GG了，用一用就没了，怪费IP的；V2Ray号称无敌然而我就没成功过= =；现在用SSR是可以的； 估计是校园网的问题吧，大家可以都试试 写在前边【基本配置条件介绍】介绍一下我的基本配置信息 我的VPS是在vultr上购买的，买最便宜的套餐就好了。但是买只有ipv6的好像有点问题，推荐还是找$3.5的购买。现在是在New York (NJ) 这个地区有。当然也可以买日本/新加坡的，就是要稍微贵点，延迟要低很多(200+ms -&gt; 100-ms)，但是这两个地区的IP被封的比较严重。大家自己考虑 服务端系统一般是Ubuntu 16/18，基本是一样的，为了稳定的话还是选用16吧，现在支持16的比较多（本文时间是 2019/12/20哈） PC客户端我肯定是用在win10下的； 手机客户端我是iOS，用美区账号下载的Potatso Lite，支持SS和SSR。但是！！不支持V2ray，v2ray这个东西的iOS客户端都要花钱，emmm反正我也不能用，就直接放弃了 ShadowSocks服务端小飞机主要是使用teddysun的部署脚本 1wget –no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh 然后加个权限，然后执行脚本部署 12chmod +x shadowsocks.sh./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 客户端客户端的话，最好直接从官网下载 1https://github.com/shadowsocks/shadowsocks-windows/releases 给个4.1.8的链接： 1https://github.com/shadowsocks/shadowsocks-windows/releases/download/4.1.8.0/Shadowsocks-4.1.8.0.zip SSR服务端SSR也是使用部署脚本 1wget --no-check-certificate https://freed.ga/github/shadowsocksR.sh; bash shadowsocksR.sh 也是一样，全程指导，最后打印信息，无需赘述 客户端首选当然是github的release 1https://github.com/shadowsocksrr/shadowsocksr-csharp/releases 下不下来的话，再给个最近可以用的链接 1http://39.105.118.158:8083/ssr/windows/ShadowsocksR-win-4.9.2.zip 要是到时候不给用了就去github再找找吧 = = V2Ray服务端命令行部署脚本【首选】一个是命令行的部署脚本 1bash &lt;(curl -s -L https://git.io/v2ray.sh) 这个一定是首选，因为下边那个可能有点问题 部署以后可以使用一些命令，比如： 1v2ray url # 导出链接 web界面部署【可以尝试】另一个比较舒服的是这个项目：https://github.com/FunctionClub/V2ray.Fun；给了一个面板可以操纵，给个图看看 舒服的很哈，但是好像有点问题，给出部署命令 1wget -N --no-check-certificate https://raw.githubusercontent.com/FunctionClub/V2ray.Fun/master/install.sh &amp;&amp; bash install.sh 客户端推荐去github官网下载，不在v2ray组织下，应该是一个fork出来的项目= = 1https://github.com/2dust/v2rayN/releases 记得下载v2rayN-core的版本哈，给个3.2版本的链接 1https://github.com/2dust/v2rayN/releases/download/3.2/v2rayN-Core.zip 我下载以后偶尔会有双击打不开的问题，可以用命令行打开。 部署BBR加速这个加速我是不知道有么有用的，一直没什么体会； 应该SS下是可用的，在SSR和V2Ray下我就不知道了，把命令放出来吧 123wget --no-check-certificate https://raw.githubusercontent.com/wn789/BBR/master/bbr.shchmod +x bbr.sh./bbr.sh 关于在Linux下使用客户端的用户别用！！！ 回忆起我用命令行的梯子真是血泪史。 真要用自己去查吧，命令行下的确实是比较诡异的，不如Win下开箱即用]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>梯子</tag>
        <tag>SS</tag>
        <tag>SSR</tag>
        <tag>v2ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git实现原理]]></title>
    <url>%2Fposts%2Fgit%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-2019-12-13%2F</url>
    <content type="text"><![CDATA[今天学习了一点git的实现原理，稍微记录一下 before all我们先来新建一个测试仓库，就叫test吧，使用git init来初始化这个仓库。 现在这个仓库是全空的，打开.git文件夹，发现里边有文件夹，但是objects啊refs啊都是空的； 1234567...+---objects| +---info| \---pack\---refs +---heads \---tags 我们就从这个仓库开始讲吧 git存储原理我们新建几个文件试一下，新建3个文件，a.txt，b.txt，c.txt，其中a和b的内容是一样的 1234$ cat a.txt b.txt c.txt1.2.3 1.2.34.5.6 然后我们使用 git add * 命令，然后再次查看.git文件夹 123456789101112+---objects| +---04| | 95c4a88caed0f036ffab0948c17d4b5fdc96c1| || +---bb| | 8dc6360fdfeabbafe0681b258a816b259a6048| || +---info| \---pack\---refs +---heads \---tags 看！多出来两个object文件，这两个文件内容是什么呢 123/d/test/.git/objects/04 (GIT_DIR!)$ cat 95c4a88caed0f036ffab0948c17d4b5fdc96c1 xK??OR0c0?3?3? N 发现不是明文的，这是因为 git将文件内容进行了压缩处理，并且加入了一些自己管理所需的信息，得到了里边这个字符串的内容； 并且git对文件内容进行了哈希，得到了这个哈希值，作为文件名。 我们可以使用git cat-file -p查看内容，使用git cat-file -t查看文件类型 1234567$ git cat-file -t 0495 &amp;&amp; git cat-file -p 0495 blob 1.2.3 $ git cat-file -t bb8d &amp;&amp; git cat-file -p bb8d blob 4.5.6 两个object文件都是Blob类型（二进制文件），内容分别是a.txt(b.txt)和c.txt的内容。 明显，这两个文件就存储了我们的文本信息，这也就是git存储的真正的文本信息。 这时候突然想到了，我们有三个文件啊，看起来很明显了，git对两个相同的文件求哈希以后发现文件相同，因此只保存了一份在数据库里。 Git三个区域git分为三个区域，分别为 工作区：就是敲代码的时候，代码都存在工作区里； 暂存区：这个就是使用了git add，然后会把这部分代码暂存起来； git仓库：这就是最终的存储地方了 未完待续]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[alias 如何带参数]]></title>
    <url>%2Fposts%2Falias%E5%B8%A6%E5%8F%82%E6%95%B0-2019-11-21%2F</url>
    <content type="text"><![CDATA[比如我要使用python的虚拟环境，我想要激活一个叫做 flask36 的环境，每次都要使用 1source flask36/bin/activate 好麻烦，所以使用了一个alias来简化这个操作 1alias ac='source flask36/bin/activate' 然后输入ac就可以了 但是这样的话，当使用其他环境时就不能用这个命令了 所以想要带个参数，直接用又不行，因为这个flask36这个东西要放在中间 所以研究了一下，可以这样用 1alias ac='func() &#123; source $1/bin/activate; &#125;; func' 然后就可以带参数了，比如： 1ac flask36 然后如果在不同的路径下也可以使用 ，比如 1ac the/path/of/the/virtual/env/flask36]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kolla-ansible安装Openstack(Ubuntu 18)全记录]]></title>
    <url>%2Fposts%2FKolla-ansible%E5%AE%89%E8%A3%85Openstack-Ubuntu-18-%E5%85%A8%E8%AE%B0%E5%BD%95-2019-11-16%2F</url>
    <content type="text"><![CDATA[使用Kolla-ansible在Ubuntu18.04上安装Openstack 当前只完成了all-in-one的部分，multinode遇到了坑，晚些再找找解决办法 [TOC] 准备工作 两台机器 Ubuntu 18.04 Server版 2 network interfaces 8GB main memory 40GB disk space 安装pip 123apt-get updateapt-get install python-pippip install -U pip 配置Ubuntu apt换源，这里换成了清华源（注意不同的Ubuntu这个url是不同的），而且如果安装的时候写明了源的话，进来应该就不用自己改了 12345678910111213141516cp /etc/apt/sources.list /etc/apt/sources.list.bkpvim /etc/apt/sources.list # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse pip源 123~/.pip/pip.conf[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple python-pip 问题 可能会遇到说cannot import main这样的错误，网上都让你直接修改pip的源码，但是我改了好几个都GG了，发现重装Pip比较简单 12sudo python3 -m pip uninstall pip &amp;&amp; sudo apt install python3-pip --reinstallsudo python -m pip uninstall pip &amp;&amp; sudo apt install python-pip --reinstall 按以上方式重装，注意python版本 允许root用户登录 12345678sudo vi /etc/ssh/sshd_config修改PermitRootLogin yes#PermitEmptyPasswords yes 无密码登录重启SSH服务service sshd restart # 或者 /etc/initd.d/sshd restart 安装一些基础依赖 1apt-get install python-dev libffi-dev gcc libssl-dev python-selinux python-setuptools 安装ansible 1apt-get install ansible 升级Ansible 1pip install -U ansible 【可选】添加到 /etc/ansible/ansible.cfg: 1234[defaults]host_key_checking=Falsepipelining=Trueforks=100 安装Docker和配置Docker hub的国内源 安装：https://docs.docker.com/install/linux/docker-ce/ubuntu/ 配置国内源推荐阿里云，点进去控制台找容器，里边有配置加速器的选项，就可以了，我用了中科大和daocloud的都不成，估计晚些会自己配一个docker registry，不然项目大规模部署的话，拉取确实太慢了 网络配置 虚拟机选择3个网卡 一个是NAT，负责连外网； 一个是主机模式，给主机上的虚拟网卡配置成了10.0.0.X/24 一个随便设置了，用来给neutron的那个配置用 Ubuntu 显示网卡 1ifconfig -a 不加-a的话，如果有未启用的网卡是不显示的，一般最好加上-a 配置netplan 1234567891011121314vim /etc/netplan/50-cloud-init.yamlnetwork: ethernets: ens33: dhcp4: true ens38: addresses: [10.0.0.3/24] routes: - to: 10.0.0.0/24 via: 10.0.0.1 dhcp4: no version: 2 注意不要配置gateway4，而是使用routes进行路由，不然会导致路由的异常，可查看route -n查看路由表 我现在的路由表是 1234567Destination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.178.2 0.0.0.0 UG 100 0 0 ens3310.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 ens3810.0.0.0 10.0.0.1 255.255.255.0 UG 0 0 0 ens38172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.178.0 0.0.0.0 255.255.255.0 U 0 0 0 ens33192.168.178.2 0.0.0.0 255.255.255.255 UH 100 0 0 ens33 192.168.178.2是通外网的，设为默认路由 启用 1netplan apply 使用ifconfig测试正确性 编辑/etc/hosts文件，添加解析 安装Kolla-ansible【选】安装kolla-ansible的部署版 安装kolla-ansible 1pip install kolla-ansible 拷贝文件到工作目录 Copy globals.yml and passwords.yml to /etc/kolla directory 1cp -r /usr/local/share/kolla-ansible/etc_examples/kolla /etc/ 拷贝Inventory到当前目录 1cp /usr/local/share/kolla-ansible/ansible/inventory/* . 【荐】安装kolla-ansible开发模式 clone 两个仓库 12git clone https://github.com/openstack/kollagit clone https://github.com/openstack/kolla-ansible 安装requirements.txt 12pip install -r kolla/requirements.txtpip install -r kolla-ansible/requirements.txt 复制配置文件到 /etc/kolla 12mkdir -p /etc/kollacp -r kolla-ansible/etc/kolla/* /etc/kolla Copy the inventory files to the current directory. 1cp kolla-ansible/ansible/inventory/* . 如果要使用Kolla-ansible命令的话，就要进kolla-ansible/tools目录下使用kolla-ansible文件执行 杂项 新建用户 123useradd csdnusermod -s /bin/bash csdnusermod -d /home/csdn csdn 添加到sudoers 123456chmod u+w /etc/sudoersvim /etc/sudoerscsdn ALL=(ALL) ALLchmod u-w /etc/sudoers 用户可能对自己的目录没有权限 12chown -R csdn /home/csdnchgrp -R csdn /home/csdn]]></content>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 206. Reverse Linked List]]></title>
    <url>%2Fposts%2FLeetcode-206-Reverse-Linked-List-2019-10-09%2F</url>
    <content type="text"><![CDATA[题目Reverse a singly linked list. Example: 12Input: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULLOutput: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL Follow up: A linked list can be reversed either iteratively or recursively. Could you implement both? 简单来说，将一个链表反转过来（还推荐使用迭代和递归两种方法来实现） 题解迭代法代码123456789101112131415161718192021/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; ListNode* prev = nullptr; while (head) &#123; ListNode* tmpNext = head-&gt;next; head-&gt;next = prev; prev = head; head = tmpNext; &#125; return prev; &#125;&#125;; 分析画图分析： 每个部分的蓝色是我们预先定义的一些变量，下边的 红色-&gt;黄色-&gt;绿色 是我们反转链表的步骤（标注了1，2，3以方便理解）。 递归法代码123456789101112131415161718/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if (head == nullptr || head-&gt;next == nullptr) return head; ListNode* node = reverseList(head-&gt;next); head-&gt;next-&gt;next = head; head-&gt;next = nullptr; return node; &#125;&#125;; 分析简单来说，递归法的思路是这样的 首先我们将头部后边的反转； 然后我们将后边的尾巴连接到头上； 然后把头的next指向nullptr；]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Linked List</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMake 从0开始]]></title>
    <url>%2Fposts%2FCMake-%E4%BB%8E0%E5%BC%80%E5%A7%8B-2019-09-06%2F</url>
    <content type="text"><![CDATA[从CMake是什么讲到如何写一个简单的CMake工程，再到CMake的一些常用的变量和函数介绍。 希望可以教会大家入门CMake。 [TOC] CMake是什么CMake的全称是：Cross Platform Make，即跨平台Make；CMake到底是什么呢？WikiPedia的解释是这样的： CMake是个一个开源的跨平台自动化建构系统，用来管理软件建置的程序，并不依赖于某特定编译器，并可支持多层目录、多个应用程序与多个库。 :link: 链接：https://zh.wikipedia.org/wiki/CMake 比较难以理解吼，使用过命令行编译或者Unix的Makefile进行构建的同学应该大概可以类比一下，CMake就是一个写脚本然后可以生成Makefile一类的文件的程序。 只用IDE进行过工程开发的同学可能不太能理解，但是可以注意一下：大部分IDE需要新建工程，新建工程后会多出来一些工程文件（比如Eclipse，Visual Studio……都会自己建），里边描述了依赖的头文件路径啊，依赖的库文件的路径啊，文件之间的依赖关系啊什么的，这其实就是CMake会生成的东西。 所以简单地说， CMake是用来进行构建的一个东西（随便你叫它语言也好、程序也好，反正就是这么一个东西），写CMake的脚本，就可以生成Makefile，就可以用Unix的Make进行编译了。 听起来还挺酷的！那么问题来了，怎么写这些东西呢？、 首先我们来介绍一下CMake的文件编排方式和结构。 CMake工程的文件和结构CMake的文件大概可以分为几类： CMakeLists.txt：描述构建信息。CMake中负责构建的文件都命名为 CMakeLists.txt。通常来说，每个模块处于一个单独的文件夹中，这样的每个文件夹都有一个专职的CMakeLists.txt负责描述它的构建需求。 .cmake文件：通常会放一些函数啊，或者专门负责定义一类变量什么的，可以在CMakeLists.txt中用include命令调用这一类文件，会顺序执行其中的语句。 也就是说，一个项目的工程目录可能是这样的： 12345678910111213141516171819RootDir├─CMakeLists.txt # 一个根目录下的顶层CMakeLists.txt├─cmake/ # cmake文件夹，存放一些构建需要的脚本什么的| ├─utils.cmake # 一个.cmake文件，utils表示存放了一些工具性的函数├─build/ # build文件夹，用作存放生成构建中间+最终文件的目录├─server/ # 一个子模块文件夹，比如：服务器程序，Server│ ├─CMakeLists.txt # 子模块通常也要一个CMakeLists.txt| ├─include/│ | ├─a.hpp│ | └─b.hpp│ └─src/│ ├─a.cpp│ └─b.cpp└─client/ # 另一个子模块，比如：客户端程序，Client ├─CMakeLists.txt # 另一个子模块的CMakeLists.txt ├─include/ | └─c.hpp └─src/ └─c.cpp 根目录下的CMakeLists.txt是整个CMake程序的入口，而子目录下的CMakeLists.txt会编译出两个可执行文件来（当然 那么CMakeLists.txt怎么写呢？ 写一个顶层的CMakeLists.txt文件顶层CMakeLists.txt文件是整个CMake构建的入口，因此通常来说会设置一些全局通用的变量啊，还要将所有的子目录包含进来。 顶层文件比子目录下的CMakeLists.txt多出来的（也是必不可少的）是如下的两句话： 123456cmake_minimum_required(VERSION 3.12)# cmake_minimum_required(VERSION major[.minor[.patch[.tweak]]] [FATAL_ERROR])project(M_PROJECT C)# project(&lt;PROJECT-NAME&gt;# [VERSION &lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;[.&lt;tweak&gt;]]]]# [LANGUAGES &lt;language-name&gt;...]) 然后我们可以使用add_subdirectory来添加我们的子目录，比如： 12add_subdirectory(./client)add_subdirectory(./server) 这样，CMake就会自动的读取这两个子目录下的CMakeLists.txt文件，然后进行解析。 使用CMakeLists.txt描述一个构建目标在子目录下的每个CMakeLists.txt文件一般会用来描述一个具体的构建目标（当然，如果工程比较简单的话，直接放在顶层的CMakeLists.txt里也是完全OK的）。 我们来举个简单的栗子，比如如下的这条编译命令： 1$ g++ main.cpp lib.cpp -o demo -I../include -lpthread -DA_PREDEF_MACRO=STH 解释一下，是将两个cpp源文件一起编译成一个可执行文件，叫做demo；我们还给这个编译命令添加了一个包含目录，是../include；还有一个预定义的宏，即A_PREDEF_MACRO，值为STH；同时还指定了一个链接库，即pthread； 我们把这个改编成一个CMakeLists.txt来看看。 首先，我们需要明确，我们的构建目标是一个可执行文件，叫做demo。所以，我们需要添加一个target。 这里的目标（target）是一个很重要的概念，不管是CMake还是Makefile，整个构建都是围绕着target来的。这个target可以是可执行文件，也可以是一个.a的静态库文件，或者.so的动态库，都是可以的。 这个target肯定需要指定源文件，因此CMake将添加源文件的操作与添加target放在一起，如下： 123add_executable(demo main.cpp lib.cpp) # &lt;=&gt;# g++ main.cpp lib.cpp -o demo 然后我们处理我们加入的其他的选项，比如添加包含目录-I../include，如下： 123target_include_directories(demo PRIVATE ../include)# &lt;=&gt;# g++ ... -I../include 我们还给他添加了链接库，链接到pthread库-lpthread，如下： 123target_link_libraries(demo PRIVATE pthread)# &lt;=&gt;# g++ ... -lpthread 我们还添加了编译期定义的宏变量-DA_PREDEF_MACRO=STH，如下： 12345target_compile_options(demo PRIVATE -DA_PREDEF_MACRO=STH)# OR# target_compile_definitions(demo PRIVATE A_PREDEF_MACRO=STH)# &lt;=&gt;# g++ ... -DA_PREDEF_MACRO=STH 把上边这几句话组合到一个CMakeLists.txt文件中，就完成了对如何编译demo这个可执行程序的完整描述。 CMake的一些基础语法显示帮助信息 message()当需要打印信息时，我们会使用这个函数，举个栗子： 1234message("Hello,world")message(STATUS "A status code") # STATUS 会在打印消息前加上两个-，比如 "-- ..."message(AUTHOR_WARNING "A user warning") # AUTHOR WARNING 会把这条消息按照警告的格式输出message(FATAL_ERROR "An error") # FATAL_ERROR 会发送一个error然后退出程序 其实还有很多其他的标记，去看看文档吧：https://cmake.org/cmake/help/v3.3/command/message.html?highlight=message 变量变量一般会用set来设置，比如 1set(target demo) 这样，我们就定义了一个变量，叫做target，其值为demo。 那么如何访问这个变量的值呢？这一点上cmake和shell是非常相似的，我们可以通过${}来访问。比如： 123message(STATUS $&#123;target&#125;) # 打印：-- demo# ORmessage(STATUS "$&#123;target&#125;") # 加双引号不影响解析，还是打印：-- demo 给${target}外边加个””是没有影响的，只是将其变成了一个真·字符串。 如果需要使用列表的话，有两种不同的写法，比如： 123set(aList "demo1;demo2;demo3")# orset(bList demo1 demo2 demo3) # 完全等价 条件语句就是if啦，这是一个简单又不简单的语句，举个栗子： 12345if (A EQUAL B) message("A equal to B")elseif(A EQUAL C) message("A equal to C")endif() if里的那个判断语句只是一个简单的栗子，CMake还提供了好多的判断方式，多给几个栗子尝一尝： 1234if(IS_DIRECTORY ../include) ...if($&#123;aString&#125; MATCHES regex) ...if($&#123;aString&#125; IN_LIST $&#123;a_list&#125;) ...... 好多好多，具体去看官网的说明吧：https://cmake.org/cmake/help/v3.3/command/if.html?highlight=#command:if foreach循环循环语句也好理解吼，举个栗子： 123foreach(element IN LISTS a_list) message("$&#123;arg&#125;")endforeach() 简单的循环了一个列表，foreach还有不少其他的遍历方式，比如： 123foreach(loop_var arg1 arg2 ...) ...foreach(loop_var RANGE total) ...foreach(loop_var IN ITEMS item1 item2) ... 还是蛮好理解的，也就不多做解释了 老规矩，看官网：https://cmake.org/cmake/help/v3.0/command/foreach.html?highlight=foreach 函数函数有两种写法，一种是macro，另一种则是function。其实效果上是差不多的，我们都来举几个栗子。 1234567891011121314151617181920macro(m_macro cpu_type linux_name) # 假设就按照 m_macro(x86, ubuntu, a_unknown_parameter) 这样来调用 message("$&#123;cpu_type&#125;, $&#123;linux_name&#125;") # x86, ubuntu # ARGC 保存了传入的参数数量 message("$&#123;ARGC&#125;") # 3 # 可以使用 ARGV#n 来访问第n个参数 message("$&#123;ARGV0&#125;") # x86 # ARGN 保存了我们在声明中没有声明的变量，就是最后的几个 message("$&#123;ARGN&#125;") # a_unknown_parameter # ARGV 保存了所有的参数列表 message("$&#123;ARGV&#125;") # x86;ubuntu;a_unknown_parameterendmacro(m_macro)function(m_function cpu_type linux_name) # 假设就按照 m_function(x86, ubuntu, a_unknown_parameter) 这样来调用 message("$&#123;cpu_type&#125;, $&#123;linux_name&#125;") # x86, ubuntu # 和macro一样，也有那几个参数的控制 message("$&#123;ARGC&#125; &amp; $&#123;ARGV0&#125; &amp; $&#123;ARGN&#125; &amp; $&#123;ARGV&#125;") # 3 &amp; x86 &amp; a_unknown_parameter &amp; x86;ubuntu;a_unknown_parameterendfunction(m_function) macro的官网说明：https://cmake.org/cmake/help/v3.0/command/macro.html function的官网说明：https://cmake.org/cmake/help/v3.0/command/function.html 根据官网的说法，在我们调用函数时，会首先将函数里定义的命令里的变量替换为我们传入的值，然后顺序执行所有的命令。 当然，函数和宏还是有一定的区别的，从官网找到的差别大概是函数会打开一个新的命名作用域， 而宏的话则是一个纯粹的替换，和上下文使用的作用域是完全相同的。这也是和我们平常使用的宏和函数是可以类比的。应该还是可以理解的。 这里有一篇文章讲的非常好，可以看一哈：https://juejin.im/post/5a8ab0e4f265da4e9d223972 find_package函数 这是一个感觉需要讲的东西，用来寻找依赖的库 这个应用的场景通常是在我们需要自己指定依赖的第三方库时，比如我们需要依赖哪个公司提供给我们的一个so文件，我们会把这个so存放在我们的一个目录下（比如，openGL的libGL.so，在windows下叫做opengl.lib)，那么如何让我们的CMake程序找到这个依赖的so文件呢？这时候就要用到这个命令了。 我们先来回顾一下之前是如何写依赖库的，我们使用的是target_link_libraries()命令。 12add_executable(demo main.cpp lib.cpp) target_link_libraries(demo PRIVATE pthread) 这个pthread是系统库，不需要进行进一步的描述，如果不是系统自带的库的话，理所当然的，我们需要告诉CMake这个库所在的位置。比如使用libGL.so时，我们直接写 1target_link_libraries(demo PRIVATE GL) CMake十有八九是很懵逼的，什么GL，哪里来的GL？因此我们可以使用一个如下这样的命令告诉CMake这个库是什么，在哪里存放着。 find_libraries函数我们之前说过了一个很重要的概念叫target，我们可以编译出一个可执行文件或者一个库（lib或者so什么的）作为target。CMake也提供了一组函数，供我们导入现有的库作为CMake里的target。下面我们来举栗子： 1234find_libraries(lib_gl GL ../libs) # 会去../libs目录下寻找名为libGL.so的文件， # 然后放在lib_gl这个变量里add_library(opengl SHARED IMPORTED GLOBAL IMPORTED_LOCATION $&#123;lib_gl&#125;) # 表明是导入的库，会直接导入位置指定的文件作为目标 重点就是上面的哪个IMPORTED，表明是导入的库，无需编译构建。 这样我们就可以使用opengl这个名字作为依赖库了，举个栗子尝一尝： 12345find_libraries(lib_gl GL ../libs)add_library(opengl SHARED IMPORTED GLOBAL IMPORTED_LOCATION $&#123;lib_gl&#125;)# 然后就可以使用了target_link_libraries(demo PRIVATE opengl) 就可以顺畅的将这个添加到我们的工程里。 一种更通用的写法通用的写法就是使用find_package命令了。 当我们使用的库比较多的时候，比如一个产品会提供好多个so文件，那么此时我们通常会使用一个类似命名空间的方法来控制，比如： 1target_link_libraries(demo PRIVATE opengl::GL) 即将Opengl这个大命名空间下的GL库作为依赖。 随便怎么理解啦，理解为一个叫做“opengl::GL”的库大概也是可以的 find_package(XXX ...)命令会去一些目录里寻找一个命名为findXXX.cmake的脚本文件并运行它，我们通常会在这样的一个文件里写我们的寻找库文件的具体过程。 这些目录被定义在CMAKE_MODULE_PATH里，我们可以自己编辑这个变量，添加我们自己的目录进去 这个文件是可以完全由我们自己定义的，里边写什么都是很随意的了，CMake没有对这个做什么很严格的要求。但是通常来说，我们会定义以下的一些变量： XXX_FOUND：表明包已经找到了，完全OK，在下文中可以正常使用； XXX_LIBRARIES：一个列表，写明了所有导入的库在cmake中的名称； XXX::lib_name：每个都是一个单独的库文件，比如我们上面提到的opengl::GL； 还有一个常用的函数是：FindPackageHandleStandardArgs，方法如下： 123FIND_PACKAGE_HANDLE_STANDARD_ARGS(NAME [REQUIRED_VARS &lt;var1&gt;...&lt;varN&gt;])# 举个栗子find_package_handle_standard_args(OPENGL REQUIRED_VARS lib_GL) 当然这个方法的参数还有很多，请看官网链接：https://cmake.org/cmake/help/v3.0/module/FindPackageHandleStandardArgs.html 这个函数做了什么呢？其实就是验证和lib_GL放一起的这些变量有没有都找到，如果都找到的话，就会把OPENGL_FOUND设置为真，否则会报错表示缺少了一些需要的库。 具体找包的过程和上述的find_libraries是一样的，也就不太赘述具体的过程了，直接给个栗子尝尝（请详细的看下注释）： 12345# findOPENGL.cmakefind_libraries(lib_gl GL ../libs)find_package_handle_standard_args(OPENGL REQUIRED_VARS lib_GL)add_library(OPENGL::GL SHARED IMPORTED GLOBAL IMPORTED_LOCATION $&#123;lib_gl&#125;) 到了我们真正的CMakeLists.txt文件中，栗子如下： 12345678...find_package(OPENGL REQUIRED GLOBAL)...if (NOT OPENGL_FOUND) ...endif()...target_link_libraries(demo PRIVATE opengl::GL) 实际中，我们通常会在顶层文件中把所有依赖的包都找出来，会比较好统一管理 CMake常用变量CMake内置了很多变量，比如我们上一节提到的CMAKE_MODULE_PATH就是一个，还有一些常用的，我们直接列下来： 全局编译选项 CMAKE_C_FLAGS：编译C程序时加入的编译器选项； CMAKE_CXX_FLAGS：同理，编译C++程序时的编译选项； 工作目录信息 CMAKE_CURRENT_SOURCE_DIR：当前CMakeLists.txt所在的目录 EXECUTABLE_OUTPUT_PATH：输出可执行文件的目录 LIBRARY_OUTPUT_PATH：输出库文件的目录 CMAKE_CURRRENT_BINARY_DIR：当前正在编译的目标要输出的目录 以上的这些变量通常都可以自己设置，比如可以自己指定可执行文件输出到哪里，自己指定要使用哪些全局的编译选项…… CMake版本 CMAKE_MAJOR_VERSION，CMAKE 主版本号,比如 2.4.6 中的 2 CMAKE_MINOR_VERSION，CMAKE 次版本号,比如 2.4.6 中的 4 CMAKE_PATCH_VERSION，CMAKE 补丁等级,比如 2.4.6 中的 6 系统信息 CMAKE_SYSTEM，系统名称,比如 Linux-2.6.22 CMAKE_SYSTEM_NAME，不包含版本的系统名,比如 Linux CMAKE_SYSTEM_VERSION，系统版本,比如 2.6.22 CMAKE_SYSTEM_PROCESSOR，处理器名称,比如 i686. CMake常用方法message函数最常用的当然就是我们提过的message啦，不在此处赘述了 FILE函数file方法可以对文件系统进行很多操作，比如读写，新建，重命名，哈希校验……具体的还是要看文档 https://cmake.org/cmake/help/v3.0/command/file.html?highlight=file 有一个方法GLOB是我曾经踩过的坑，在这里提一嘴 1file(GLOB variable [RELATIVE path] [globbing expressions]...) 这里的globbing expressions和正则很像，但是真的不是正则，可以到维基上看一下什么叫globbing，千万不要简单的当作正则来用了。 INSTALL方法这个东西一说就很多了，但是也不是很难，大家直接看官网就好了 The end but may be not the end感觉想到的东西都已经说的差不多了，那就先到这里吧，想起来了会继续补充的，希望大家栗子吃的开心，也学到了一些CMake的小技能]]></content>
      <categories>
        <category>CMake</category>
      </categories>
      <tags>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式互斥算法]]></title>
    <url>%2Fposts%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5%E7%AE%97%E6%B3%95-2019-07-08%2F</url>
    <content type="text"><![CDATA[本文记录了三个分布式互斥算法，主要解决的是分布式系统中对临界区的访问控制 Lamport 算法 Ricart-Agrawala 算法 Singhal 算法 分布式互斥算法的基本概念要通过消息传递，来控制哪个进程可以访问临界区资源 性质 安全性：不会造成多个进程同时在临界区执行 活性：不会死锁，不会饿死 公平性：排序基本达到FIFO，先到先进 Lamport 算法 最简单的算法，老爷子先占了个名字哈哈哈，老师上课吐槽这老爷子早期疯狂抢山头，用这么简单的算法混上了冠名 请求临界区时：向所有其他进程发送 $REQUEST(t_i, i)$，将这个请求保存到本地请求队列里； 如果收到了一个请求，那么按时间戳将请求加入到本地请求队列，然后返回一个 $REPLY(t_j)$，也是当前的时间戳； 发送了请求以后进程就等啊等啊，直到 收到了所有其他进程返回的 $REPLY(t*)$，期望是对本地的 $t_i$进行相应，所以要求 $t* &gt; t_i$，也就是说，是我发了消息后，对方才回复的消息； 并且本地的 $t_i$ 在本地队列的队头 那么就执行，可以进入临界区 退出临界区时，从本地队列里删掉自己的请求，带着时间戳发一个广播，通知所有队列都可以删掉$i$ 请求了； 很明显是要求FIFO通道的，想象一下连续发两个请求，如果非FIFO的话，返回值可能会乱序， 此时收到回复B，就将A送进去了，收到回复A时，发现B在对头，但是回复A的时间戳比B小，不给进入临界区，算法就卡死在这里了； Ricart-Agrawala 算法 主要就是来优化上一个算法里的消息通信机制的 上一个算法的冗余主要有： 没有申请临界区资源的进程是没必要知道你是否进行了 $RELEASE$ 操作的； 1.1 同样，早早的回复REPLY也是没用的，恢复了也进不去 1.2 所有，不如等到RELEASE的时候再REPLY 同样，其实也是没必要知道你是否要申请临界区资源的，这和它没有任何关系； 这个算法就针对1进行了优化 进入临界区没变化，还是要广播 $REQUEST(t_i, i)$； 收到请求时： 如果接收方发现自己没有请求临界区，也没有执行临界区，那就直接$REPLY$； 如果接收方正在请求临界区，但是请求的时间戳比请求消息的时间戳的要大，也就说，比收到的请求消息申请的要晚，那也就直接 $REPLY$ 如果都不是，也就是说，接收方收到这个请求之后，发现自己需要比这个请求更早的进入临界区，那就先不回复它，而是记录本地的一个数组 $RD[i] = 1$，就是说，自己延迟了$p_i$进程的1个请求； 同样，收到所有的 $REPLY$ 之后，才能进入临界区； 退出临界区时，要按照 $RD$ 的记录，将延迟的 $REPLY$ 都发送出去； 这个算法优化了什么呢？ 可以看出来，去掉了 $RELEASE$ 消息，$RELEASE$ 不需要发送了； 为什么呢？根据我们前边的分析，将$RELEASE$发送给那些未申请临界区的进程是没有必要的，因此我们不想给他们发送，而申请临界区的进程就是给我们发送了$REQUEST$ 消息的进程。因此我们只需要给我们需要$REPLY$的那些进程发送$RELEASE$就可以了。 继续分析一下，其实马上回复$REPLY$也是没必要的，反正回复了它也不能马上进行访问，不如等到它可以访问的时候再回复他（对请求方进程来说没差别，对吧）。 基于这样的想法，我们把 $REPLY$ 延迟到释放资源时再发送。我们可以发现，但凡是我们发送 $REPLY$ 时，都没有在继续占用临界区资源了，也就是说，将这两个消息合并成了一个消息，减少了 1/3 的消息复杂度。 Singhal 自适应算法 听名字就不一般，自适应算法，肯定是很牛了 这个算法还是根据上面来继续优化的，我们上面分析到了冗余2，但是没有进行解决，这里就是来处理这个问题的 算法的思路很精妙，一个经常请求临界区资源的进程其实是没必要获得那些不请求临界区资源的进程的同意的，给他们发 $REQUEST$ 消息也只是浪费带宽（还要收一下回复，真浪费啊）。 所以我们把进程分为低频和高频的部分，按序排列，请求临界区时，只需要获得更高频率的进程的同意就可以了。 思想很简单，但是漏洞蛮大的，肯定要在细节上处理很多小问题，我们继续看算法。 定义如下的变量： 请求集合$Ri$ ：进程𝑆𝑖执行临界区之前，必须获得该集合中所有进程的许可 通知集合$Ii$：进程𝑆𝑖退出临界区之后，必须通知该集合中所有进程 逻辑时钟$C_i$ ：每个进程维护一个Lamport时钟 布尔变量 𝑅𝑒𝑞𝑢𝑒𝑠𝑡𝑖𝑛𝑔：当进程正在请求临界区时置为1 𝐸𝑥𝑒𝑐𝑢𝑡𝑖𝑛𝑔 ：当进程正在执行临界区时置为1 𝑀𝑦𝑝𝑟𝑖𝑜𝑟𝑖𝑡𝑦 ：若进程正在执行临界区的请求比当前申请进入临界区的请求具有更高的优先级，则置为1 请求时： 先设置自己的 $Requesting = true$，正在请求； 然后逻辑时钟递增； 向 $R_i$ 发请求，并且等待所有的回复（收到回复时，将对方从R里删除，更新时钟）； 请求完毕了（允许进入了），设置 $Requesting=false$； 收到消息时： 更新时钟（max）； 如果当前正在请求临界区，那么根据优先级: 如果自己优先，就把对方加入到通知队列（$I_i$）里，等下通知他可以执行； 如果对方更高，那就要先回复对方，然后把对方加入到自己的请求队列（$R_i$）里去，并且向人家补一个请求（之前没请求，所以要补）； 如果正在执行临界区，那就没办法了，把对方加入到通知队列里，等下通知； 如果既不请求，也不执行，那么就放到请求队列里，然后马上回复对方； 执行： 设置 $Excuting = true$，正在执行； 执行，然后设置为 $False$； 释放临界区： 对 $I_i$ 里的所有进程，发 $REPLY$ 消息； 发完了就把这个加到 $R_i$ 里去； 需要加入初始化来规避一些问题，我们初始化Ri只包含$P_{1..i-1}$，在动态调整的过程里，也要保证要么 $S_I \in R_j$ ，要么 $S_j \in R_i$；]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Raynal-Schiper-Toueg因果序保证算法]]></title>
    <url>%2Fposts%2FRaynal-Schiper-Toueg%E5%9B%A0%E6%9E%9C%E5%BA%8F%E4%BF%9D%E8%AF%81%E7%AE%97%E6%B3%95-2019-07-07%2F</url>
    <content type="text"><![CDATA[又是一篇没人写的算法，这门课没得参考就很尴尬 是一种分布式环境下的因果序保证算法 Pre: 消息通道，因果序 因果序是什么一个消息通道保证因果序，意味着当两个消息的发送有依赖关系时，接收也要有依赖的关系，即：逻辑先发的一定会在逻辑上先收到。 数学一点，是这样的： 对任意两个发向同一进程的消息 $m{ij}$ 和 $m{kj}$ ，假设 $Send(m{ij})\rightarrow Send(m{kj})$ ，则 $Recv(m{ij})\rightarrow Recv(m{kj})$ ; 也就是说，发送是有逻辑依赖的关系，在接收时也是按照逻辑依赖的顺序接收的。 除了因果序，消息通道还有以下的顺序 FIFO模型：信道运行为一个先进先出的队列模型 非FIFO模型：信道为一个集合，发送者向里面添加消息，接受者在里面移除消息，添加和移除是无序的 因果序应用：三副本机制 不再罗嗦了，课件非常清晰 保证因果序的基本思路基本思路很简单 每条消息M都携带一条日志，保存了所有因果关系上在M之前的消息； M到达目的地是，先缓存自己，然后检查自己日志里的消息是否都到达了这个目的地（只检查目的地也是这里的就可以了）； 如果满足这个要求，那么就可以把自己交付给这个目的进程了；否则要进入等待的状态； 这里有一点小问题，如果记录因果上先于M的消息呢？ 很简单，如果m2依赖于m1，m3依赖于m2。那么m2携带的日志里有m1；而m3只需要复制m2的日志，再加入m2就可以了。 也就是说，将所有已发送的消息沿着因果路径不断转发就可以了。 Raynal-Schiper-Toueg算法 注意：这个算法要求FIFO通道 算法思想基本思路是对我们上面所说的进行了简化，上面的描述中，记录了所有的消息，带来了很大的冗余。 我们考虑在FIFO通道下。当m2收到时，m1一定也已经收到了。进一步思考，发送者要求这条消息依赖于之前的，i-&gt;j的10条消息，如果j进程发现自己当前已经收到了来自i的20条消息，那么可以肯定，要求的那10条消息肯定包含在我们已收到的20条消息里（FIFO的特性，i-&gt;j通道上不会出现乱序）。 也就是说，我们只需要记录我们依赖的通道上的消息数量就可以了，这样就简化了传递的消息复杂度。 算法详述算法详细描述如下： Send[j,k]记录本进程所知的进程j发送到进程k的消息数量 DELIV[j]记录由进程j发来的、已经Deliver到本进程的消息数 发送消息时： i-&gt;j发送消息，表明所有依赖的消息是自己当前的SENT矩阵，附带在消息里发送，然后SENT[i, j]++； 接受消息时： j-&gt;i发送消息，接收到j的ST矩阵，就是当前消息的依赖，我们判断我们收到的消息数量（x是每个进程编号） DELIV[x] \geq ST[x, i]满足时，确认已收到了所有依赖的消息 然后对SENT按位取max，自身和当前消息的依赖都要满足； 然后DELIV[j]++，这是接收到的来自j的消息数量；]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异步Floyd-Warshall算法]]></title>
    <url>%2Fposts%2F%E5%BC%82%E6%AD%A5Floyd-Warshall%E7%AE%97%E6%B3%95-2019-07-06%2F</url>
    <content type="text"><![CDATA[异步的Warshall算法找了半天没找到参考，自记一篇 同步算法同步的Floyd-Warshall算法还是很简单的，算法过程如下 也很好理解，就是对于每条路径 (s-&gt;t)，寻找他们经过的最近的那个中间点（pivot），目标是找到一个s-&gt;pivot-&gt;t的路径比当前确定的s-&gt;t的路径还要近。所以就不停的测试，从s到t的路径变成s-&gt;1-&gt;t，s-&gt;2-&gt;t……会不会近一点…… 更深入来说，其实是一种动态规划的思想，先找到任意两个节点通过节点1的最短长度，然后再找通过节点2的最短长度=min{using 1, 2} 可以去百度一下，同步算法的博客有很多，讲的比我详细的多，直接搜floyd-warshall就可以了 异步算法异步算法就比较奇怪了，首先我们使用同步算法的依据是LENGTH这个变量保存了所有的距离信息，也就是说，全局的距离信息都是已知的，然而到了异步版本，每个进程都只知道自己到各个节点的距离（标记为LEN，而且这个最近的一跳也要记下来，标记为PARENT），而不知道其他两个节点的距离，所以我们需要处理，让每个进程知道全部的消息。 还是要按照同步算法的思想来处理，所以循环的最中心最重要的变量是pivot到各个节点的距离，所以我们可以将pivot节点的LEN信息对全网进行广播，所以其他节点就可以根据此个pivot的信息对自己的变量进行更新了。 草草的总结了一下，大致是这个意思的 依次用每个节点作为中心点pivot 对于每个邻居节点nbh 如果当前节点到pivot的最短路的下一跳是nbh，那么给nbh发送一个intree(pivot) 如果不是，那给nbh发送一个notintree(pivot) 等待每个邻居节点nbh的intree和notintree消息 如果当前节点和pivot已经找到了路径，那么 如果pivot不是自己，就从到pivot的下一跳节点请求PIVOT_ROW 对每个nbh邻居节点 如果收到了nbh的INTREE消息 如果pivot是自己，给nbh发送自己的len 如果不是，就发送自己存的PIVOT_ROW给nbh 当前就知道了自己到pivot的距离，pivot到其他节点的距离，可以更新路径 解释还是集中式的算法一样，用每个节点作为pivot进行考虑，程序开始时，所有节点都给自己的邻居节点发送INTREE和NOTINTREE消息，表明自己想要到达pivot节点，需要（或不需要）下一跳经过这个邻居节点。 然后每个节点都进入了await状态，直到收到各自的所有邻居节点发来的消息。 然后程序继续执行 如果LEN[pivot]不是无穷，也就是当前节点已经找到了和pivot相连的一条路径 如果没找到的话，那么说明当前节点和pivot不相连，也就没有更新路径的可能性了，在当前pivot阶段，程序可以结束了，等待下一次循环 继续进行判断，如果pivot是自己，那么自己的LEN就是pivot_row这个变量。如果不是呢，那么就要想办法获取这个变量（pivot节点到各个节点的最短距离），所以从离pivot更近的一个节点进行寻找，也就是说从Parent[pivot]进行寻找，等待它给我们发送一个pivot_row的信息。这里又要进入await状态了，没有这个消息我们确实寸步难行。 得到之后，我们遍历每个邻居节点nbh。如果收到了nbh的intree消息，那么说明我们当前节点是nbh到pivot节点的下一跳，也意味着我们要给它发送pivot_row消息（对照上一段），所以我们给他发pivot_row，同样，如果自己就是pivot，那就发自己的len就可以了，如果自己不是，那就发自己收到的pivot_row（上一段收到的）。 这样操作结束后，当前节点通知了所有其他需要通知的节点PIVOT_ROW消息，所以理论上来说，让所有节点都收到pivot_row消息的操作已经圆满了（至少分配给自己节点的任务已经完成了），可能需要等一等，就可以让所有节点知道合理的pivot_row了。 当然我们的当前节点没有必要等待，它可以直接进行下一步了，那就是更新自己的LEN变量（根据pivot_row和len，就相当于同步算法的length[s, t]和length[pivot, ...]） 这样，就可以让所有节点生成一棵全源最短路的图了。 总结本质上来说，这个和集中式的算法没有区别，但是为了不同进程的同步，我们加入了很多处理同步的内容，需要耐心但不是特别难。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nova 源码学习笔记]]></title>
    <url>%2Fposts%2FNova-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-06-26%2F</url>
    <content type="text"><![CDATA[How to study nova? U can follow me in this blog for the whole process. [TOC] 前置要求 安装Openstack 下载Nova的源代码 源码分析主要的代码都在nova目录下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556nova├── api # API 服务├── availability_zones.py├── baserpc.py├── block_device.py├── cache_utils.py├── cmd├── common├── compute # 处理计算资源├── conductor├── conf├── config.py├── console├── consoleauth├── context.py├── crypto.py├── db├── debugger.py├── exception.py├── exception_wrapper.py├── filters.py├── hacking├── hooks.py├── i18n.py├── image├── __init__.py├── ipv6├── keymgr├── loadables.py├── locale├── manager.py├── monkey_patch.py├── network # nova网络的实现├── notifications├── objects├── pci├── policies├── policy.py├── privsep├── profiler.py├── quota.py├── rpc.py├── safe_utils.py├── scheduler # nova的调度任务├── service_auth.py├── servicegroup├── service.py # 所有服务的基类├── test.py├── tests├── utils.py├── version.py├── virt├── vnc├── volume├── weights.py└── wsgi.py]]></content>
  </entry>
  <entry>
    <title><![CDATA[Nova cli 源码学习笔记]]></title>
    <url>%2Fposts%2FNova-cli-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-06-26%2F</url>
    <content type="text"><![CDATA[最终目标： 为Nova开发一个小组件！ 记录自己摸索的过程啦，持续更新直到完成 先看看书，再看看github上Openstack的组织仓库，找到了两个仓库 Nova：https://github.com/openstack/nova，Nova的大仓库，主要的功能实现应该都在这里了 Nova client：https://github.com/openstack/python-novaclient，命令行 cli （应该主要是Nova命令）的客户端 感觉应该是最重要的需要看的仓库了，可以看到调用命令行的过程，然后继续看看，还有一些可能会有用的仓库 Openstack client：https://github.com/openstack/python-openstackclient，命令行（Openstack命令）的客户端 Horizon：https://github.com/openstack/horizon，Horizon的实现里，想必也有不少的基于 Http 请求的Nova API的调用实例 之前已经看过了Nova的代码了，感觉没有太多的涉及API的部分，直接去看Nova Client部分吧 下载Nova client的代码，发现了 doc文件夹，打开以后发现和网站提供的文档是一致的， 就直接在网站上看了 https://docs.openstack.org/python-novaclient/latest/reference/api/index.html 网站上来就是代码了 12&gt;&gt;&gt; from novaclient import client&gt;&gt;&gt; nova = client.Client(VERSION, USERNAME, PASSWORD, PROJECT_ID, AUTH_URL) 幸好有之前跟着官网一步一步配置的经验，不然看着这参数肯定一脸懵。 想起来之前官网的配置教程给了一个 admin-openrc 的文件使用，每次调用 cli API 之前都要使用 . admin-openrc 激活一下。 找到自己的虚拟机，打开admin-openrc看一下，发现给了不少的参数 12345678export OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=****export OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 激活环境变量，测试一下使用nova flavor-list，正常输出。 把这些放到文档给的代码里试验一下，发现是不可以的，报错了，明显是我的权限验证有问题。 从源代码开始分析一下，全文搜索OS_PROJECT_，5个文件中存在，除了doc和test中以外，就只剩下shell.py这个文件了，果然，这个文件从环境中读取了OS_PROJECT_...的变量。同时发现这个文件存在main函数，一定也是全程序的执行入口。从这个文件一路按图索骥，找到了熟悉的client.Client()调用，然后惊喜的发现，居然有两个client？？？还好注释写的明白，第一个声明只是为了确定version，第二个声明才是真正的声明。 遇到了一个函数 importutils.import_class(…)，从根目录水平导入一个类，作为一个变量来使用，python真是博大精深啊…… 源码： 123&gt; return version, importutils.import_class(&gt; ​ "novaclient.v%s.client.Client" % version.ver_major)&gt; &gt; [Update]： 误会了，原来这是 Openstack 自己完成的一个函数，源码大致是这样操作的： getattr(sys.modules[mod_str], class_str) 试验了一下，和import语句导入的模块无差，算是一种动态导入的方式了 发现目录下只存在v2的子目录，因此看来是不存在使用 verison=2.x 这种操作了 现在思考一下，还有一个问题悬而未决，是这个程序是如何监听的，思考了一番也没有找到，只有一行代码感觉是相关的 1args.func(self.cs, args) 应该是自己给args加的属性，具体加了什么函数，还是不是很明确，应该是在这里执行的命令。 猜测nova命令就是使用了shell.py的命令解析，所以试着直接运行 shell.py并加命令行参数，果然是可以的。 然后再shell.py里简单的加了输出，没有问题；再然后在Client函数里加Log，发现没有输出，找了半天原因，发现是导入模块时，直接导入了已安装的包的模块，而不是当前目录下的模块，强行改造源代码，加入了输出的部分，一切正常了。 现在回头，从之前我们写的flavor-list这个命令开始进入程序分析 我们全项目匹配flavor list类似的关键字，找到了一个函数，do_flavor_list，位于novaclient-&gt;v2-&gt;shell.py，尝试着加一下log，输出了，没有问题，我们跟着这个函数的调用继续研究。 1234567def do_flavor_list(cs, args): """Print a list of available 'flavors' (sizes of servers).""" if args.all: flavors = cs.flavors.list(...) else: flavors = cs.flavors.list(...) _print_flavor_list(cs, flavors, args.extra_specs) 可以看到，关键函数是 cs.flavor.list()，我们去找一下flavor的定义（在v2-&gt;client.py) 1self.flavors = flavors.FlavorManager(self) 继续去看FlavorManager，这里可以直接右键跳转了，发现FlavorManager的list()方法是处理了一些参数，然后转发给了_list这个方法。这个类里没有_list方法了，继续去基类找，一路找到base.py的Manager类，这个类有_list方法，获得重要参数的方法是： 1234if body: resp, body = self.api.client.post(url, body=body)else: resp, body = self.api.client.get(url) 看一下api的定义，发现要一路回到self.flavors = flavors.FlavorManager(self)，这个api就是client本身，找到client实例的client属性，发现是个httpclient对象，用_construct_http_client方法构造，具体的操作流程就是发送了一个请求给/flavors，然后获得返回值，一路返回，也就是这个api的操作流程了。 基本上大部分的API都可以通过这样的流程找到对应的位置，在此也就不再赘述啦…… 下面该去看Nova的源码了，看一下这个服务端到底是如何建立起来的]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Nova</tag>
        <tag>云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use sequelize ORM in Nodejs]]></title>
    <url>%2Fposts%2FUse-sequelize-ORM-in-Nodejs-2019-06-23%2F</url>
    <content type="text"><![CDATA[记录一下如何使用Sequelize框架，包括如何放在项目里进行组织，如何使用框架进行CRUD的操作 开始之前先讲讲我的开发配置 语言：Nodejs 后端框架：KOA2 数据库： MySQL 5.6 选择一个ORM框架如何选择是个大问题，百度了一些，大家推荐的基本是这四种 TypeORM：读了读代码，是用typescript写的，风格上和Java非常像，用注解什么的，感觉还不错，备选 ORM2：看了几篇博客，好像有点小坑，还是算了 Knex：推荐使用Bookshelf.js，支持Oracle，放弃了，毕竟已经确定了是用 KOA2 （类似express的风格了） Sequelize：这个应该是应用最广泛的了，看了看文档也很齐全，中文文档就有不下五六个版本，应该资源是不缺的，而且相比起其他的更像是javascript，就是他了！ 基本大家都实现了Promise，所以基本都可以使用async / await Sequelize又是开始之前 一篇翻译的不错的中文文档：https://github.com/demopark/sequelize-docs-Zh-CN 官方给的详细的API参考：http://docs.sequelizejs.com/identifiers 一定要善用后边这个文档，我是后期才使用这个文档，感觉前边吃了不少亏 Get Started安装12npm install --save sequelizenpm install --save mysql2 # 可能要自己选择具体的数据库驱动，我们是MYSQL，就是这个了 建立数据库连接123456789101112131415161718192021222324const Sequelize = require('sequelize');const sequelize = new Sequelize('database', 'username', 'password', &#123; host: 'your ip address', dialect: 'mysql', // Or mariadb, postgres, mssql ... operatorsAliases: false, dialectOptions: &#123; // 字符集 charset: "utf8mb4", collate: "utf8mb4_unicode_ci", supportBigNumbers: true, bigNumberStrings: true &#125;, pool: &#123; max: 5, min: 0, acquire: 30000, idle: 10000 &#125;, timezone: '+08:00' //东八时区&#125;);module.exports = &#123; sequelize&#125;; 可以使用 .authenticate() 函数来测试连接. 12345678sequelize .authenticate() .then(() =&gt; &#123; console.log('Connection has been established successfully.'); &#125;) .catch(err =&gt; &#123; console.error('Unable to connect to the database:', err); &#125;); 建表和使用说时迟那时快，马上就到了最重要的一步，建表。话不多说直接上代码。 123456789101112// from task_table.jsmodule.exports = function(sequelize, DataTypes)&#123; return sequelize.define('task',&#123; task_id:&#123; type:DataTypes.INTEGER, primaryKey:true, allowNull:false, autoIncrement:true &#125;, ...... );&#125;; 这就完成了一个表的模型的定义，到使用时，我们要在其他文件中 require这个文件，如下： 1234567const TaskTable = sequelize.import('task_table');// 用查询举例async function searchTaskByID(task_id) &#123; return await TaskTable.findByPk(task_id); // find by primary key&#125; 使用时调用这个函数就可以返回根据id查询到的task对象了。 这样，最简单的使用就完成了，具体的建表和表查询的API咱们等下继续讨论。 建表之前我们已经给出了建表的一段简单的代码，现在具体的看看还可以提供什么 freezeTableName12345678return sequelize.define(&apos;user&apos;,&#123; id: &#123; type:DataTypes.INTEGER&#125;,&#125;, &#123; // 如果为 true 则表的名称和 model 相同，即 user // 为 false MySQL创建的表名称会是复数 users // 如果指定的表名称本就是复数形式则不变 freezeTableName: true&#125;); 如果为 true 则表的名称和 model 相同，即 user，为 false MySQL创建的表名称会是复数 users，如果指定的表名称本就是复数形式则不变 type详情请参考API文档里关于DataTypes类的说明 包含了一些主要使用的数据类型，我们常用的有Integer, Float, Char(), Text, Date ...，例子可以看上边的定义 访问器和设置器每个属性都可以设置 get(), set()函数，可以方便的获取一些格式化的值，比如对于时间的处理，由于不同的时间有不同的表示格式，我们可以在访问器这个级别来使用库来转换这个格式 123456updatedAt: &#123; type: DataTypes.DATE, get() &#123; return moment(this.getDataValue('updatedAt')).format('YYYY-MM-DD HH:mm:ss'); &#125;&#125; 这样，我们拿出来的值就不是标准时间 ****T****Z这种奇怪的格式，而是形如 YYYY-MM-DD HH:mm:ss 的时间了 外键约束使用 reference 可以指定外键，如下： 123456789team_id:&#123; type:DataTypes.INTEGER, allowNull:false, references: &#123; model: 'team', key: 'team_id', deferrable: Sequelize.Deferrable.INITIALLY_IMMEDIATE &#125;&#125;, deferrable是指定依赖的关系，比如添加时外部表没有该键如何处理，删除时是否要级联删除等……请查看文档的详细介绍 列属性定义的一些其他参数 参数名 可选取值 解释 备注 primaryKey ` true false` 主键 allowNull ` true false` 可以为空 autoIncrement `true false` 自增属性 查询从where开始Sequelize 提供了不少查询的API，可以直接使用，比如 123456let task = await Task.findAll(&#123; where: &#123; task_id: 1 &#125; attributes: ['task_id']&#125;)); 相当于SELECT task_id FROM task WHERE task_id=1，返回值是一个数组，里边包含了0个或多个 Sequelize 的 model 对象（也可以理解为查询结果转换为的Json对象） 基本上使用就是通过 where 语句进行限定，如果想要使用And | Or ，可以使用官方提供的Op类，如下： 12345678await Task.findAll(&#123; where: &#123; task_id: &#123; [Op.or]: [1,2,3,4] &#125; &#125; attributes: ['task_id']&#125;)); 我们回到这个返回值来看，这个对象还是和普通的Json不太一样，sequelize 为我们包装了不少东西，比如可以提供get()的访问。 关联查询感觉这是一个比较麻烦的地方，关键是资料还贼少，大概谢谢自己总结的一些地方 先上官方文档 建立关联关联要使用到几个关系，分别是BelongsTo, BelongsToMany, HasMany, HasOne 其中 BelongsTo和HasOne对应于1：1的关联 而HasMany对应于1：m的关联 而BelongsToMany对应于n:m的关联 1:1我们可以预先定义这个关联 12UserInfo.belongsTo(User, &#123;foreignKey: &apos;username&apos;&#125;)// 或 User.hasOne(UserInfo, &#123;foreignKey: &apos;username&apos;&#125;) 这两种写法都会给 UserInfo 加入一列，作为外键，指向 User 的username 上，当然如果需要这个添加反映到数据库上，需要使用 Sequelize.sync()才可以 完成以上这一步之后，当你获得从 findAll 的API得到的 UserInfo 对象时，可以使用 getUsers() 这样的函数，来获取和他关联的 user 对象。 1:m和上边差不多，只是换成了 hasMany 而已 m:n这个是面向多对多的关联的，比如用户加入小组，需要在用户和小组之间建立一个联系，大概像这样： 123| id | username | team_id | ... || 0 | user1 | 1 | ... |... 我们可以使用 BelongsToMany 这个关联，需要提供一个through的表，比如： 1Team.belongsToMany(User, &#123;through: Members, foreignKey: 'team_id', target_key: 'username'&#125;) 需要预先定义一个 Members 的 Model，可以不写东西，会自动添加两边的主键进去，也可以自己先定义一些其他的需要的列，比如状态什么的。 使用关联进行查询查询时需要使用的是include方法，如下： 1234567Task.belongsTo(User)return await Task.findAll(&#123; where: &#123; ... &#125;, include: [&#123; model: User &#125;]&#125;) 相当于先在关联的外键上做一个Join ，然后进行查询。可以看到 include 提供的是一个数组，所以是可以提供多个表级联的查询的。 可能每次在前边写 belongsTo 感觉比较奇怪，一种可选的写法是： 123456return await Task.findAll(&#123; where: &#123; ... &#125;, include: [&#123; association: Task.belongsTo(User) &#125;]&#125;) 也是差不多的 Update、Delete、Create这三个就直接一起讲了，没太大区别 12345678910111213141516171819models.TR.create(&#123; username: username, task_id: task_id, state: models.status_code.tr.WAITING_TO_BE_DONE&#125;)models.Task.update(&#123; state: models.status_code.task.ACCEPETED_AND_DOING&#125;, &#123; where: &#123; task_id: task_id &#125;&#125;) TR.destroy(&#123; where: &#123; task_id: task_id &#125;&#125;) 具体参数写了什么不需要管，反正大概就是这个意思啦，要注意删除用的是 destroy sql 聚合函数聚合函数是指 count 啊什么的，比如： [[sequelize.fn(&#39;COUNT&#39;, sequelize.col(&#39;hats&#39;)), &#39;no_hats&#39;]]，调用了count函数，对hats 列进行计数，并将结果存为no_hats，我们可以在include或findAll的参数里直接使用这个。 使用原始SQL语句到现在肯定有同学发现了，还是不灵活，有不少操作很难完成，幸好我们可以直接使用 sql 语句进行查询，如下： 123456await sequelize.query( "SELECT * FROM `task` where `task_id` NOT IN (SELECT `task_id` FROM `teamtask`) AND `publisher` = \'" + org_name + "\'", &#123; type: sequelize.QueryTypes.SELECT&#125; ).then(result =&gt; &#123; return result &#125;); 这里当然是可以进行参数绑定的，我就直接裸加了，如果需要参数绑定请自行百度一下吧哈哈（当然这个语句还是可以用两个awiat完成的，我没什么例子了随便写写啦） 一些小tips一是使用redis进行缓存，这个可以单独去搞，也就不在这里赘述了。 除此以外，我们可以尝试着让一些查询尽可能的并行化 Promise.all这里要提到一个工具：Promise.all([])，举例如下： 我们接受任务后需要对任务的状态进行修改，即这样的需求： 12await models.TR.create(...);await models.Task.update(...); 这就是2个await语句，我们可以把这个包装一下： 1234let result = await Promise.all([ models.TR.create(...), models.Task.update(...)]) 相当于把两个串行改成了并行的，还是可以省不少时间的，特别是访问多了以后 要注意每个异步语句都是会立即返回一个promise对象的，因此我们可以用循环运行所有的这些操作，并将返回值存储起来，然后放到Promise.all()的参数中，等待其完成 Transaction把所有的时间打包作为一个事务提交，理论上应该免去了网络的时延啊，虽然我没有用，但是应该是一个很好的选择。 目录结构我们项目使用的目录结构大致如下： 12345root├─......├─controllers├─models└─tables controllers负责处理router模块处理的URL后分发的请求，对参数进行分析，处理主要的业务逻辑部分； models负责与数据库进行交互，使用我们上面提到的这一堆API进行实现； tables负责建表的语句； 以上三个文件夹有明显的分层，存在由上到下的调用的依赖，同级之间尽量不发生依赖关系，也不会出现依赖上级代码的情况。如果可以的话，我也推荐在每个文件夹写一个总的导出的文件，到上一层时，使用这个文件进行导入，而不是导入每个下层文件夹里所有的文件，这样也可以保证所有的建表语句会在开始时运行，而不会出现最后发现有个表没有用到的情况。]]></content>
      <categories>
        <category>后端</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>sequelize</tag>
        <tag>nodejs</tag>
        <tag>ORM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rasterization-Bresenham, scan line method]]></title>
    <url>%2Fposts%2FRasterization-Bresenham%2C%20scan%20line%20method-2019-03-20%2F</url>
    <content type="text"><![CDATA[光栅化算法，主要是： DDA 画直线算法； Bresenham 快速画直线算法； Bresenham 画圆算法； Edge Equation 三角形填充算法； 实现结果先看效果啦，分别是使用Bresenham算法画直线、圆、并使用Edge Equation算法填充三角形的效果 DDA想了想还是先提一句DDA吧，这个也是经典了 DDA就是简单的数值微分然后拟合的算法，它的原型大致过程如下： 给两个点 $(x_1, y_1)，(x_2, y_2)$ 可以得到一个直线的表达式 $y=mx+b$ 每次推进一个$x$，即每次 $x=x+1$，然后计算 $y=mx+b$ 的值，每次要进行取整； 如果 $m&gt;1$，那么可以选择每次将 $y$ 推进1格。 通常来说，DDA对上述过程做了一定的优化，每次不需要都计算 $y=mx+b$ 了，而是选择如下的公式计算： x_{k+1}=x_k+1；\\ y_{k+1}=y_k+m；到了 $m&gt;1$ 时就换成 $x_{k+1}=x_k+\frac{1}{m}$ 就好了，计算出结果以后取整作为像素点的坐标值。 看起来好了很多，但是还是要涉及到浮点数的运算，还有不少的取整操作，对于计算机来说实在是有点困难（因为会算的很多，浮点总是要慢一些的，还会涉及到一定的舍入误差累计）。 Bresenham算法感谢Bresenham老爷子提出的这个Bresenham算法，现在这基本上是所有直线算法的标准了，一般都会直接集成到硬件里。 Bresenham 直线算法原理先上一张图 可以看出来，每次选下一个点都是在 $(x_k+1, y_k+1)$ 和 $(x_k + 1, y_k)$ 之间选取，到底选哪个要看直线到底在这个x坐标下，离哪个更近一些。 我们把上下的距离分别标注为 $ d{upper} $ 和 $d{lower}$ ，当下边比较大的时候，就要选取上边的点，所以我们让他们相减，代入直线公式可以得到 d_{lower} - d_{upper} = 2m(x_i + 1) - 2\overline{y_i} + 2B - 1我们已经知道了 $\Delta x = x{end} - x{start} $ 是一个正值，所以我们可以把这个乘到上式，不会影响正负。 p_i = 2\Delta y \cdot x_i - 2\Delta x\cdot \overline{y_i} + c 这里 $\Delta x= x{end} - x{start}， \Delta y = y{end} - y{start}，m=\Delta y/ \Delta x$ ​ $c=(2B-1)\Delta x + 2 \Delta y$； 然后我们可以代入算出来 \begin{align} \mathrm { p } _ { 0 } &= 2 \Delta \mathrm { y } \bullet x _ { 0 } - 2 \mathrm { x } \bullet \overline { y } _ { 0 } + ( 2 B - 1 ) \Delta x + 2 \Delta y \\ &= 2 \Delta y \bullet x _ { 0 } - 2 \left( \Delta y \bullet x _ { 0 } + B \bullet \Delta x \right) + ( 2 B - 1 ) \Delta x + 2 \Delta y \\ &= 2 \Delta y - \Delta x \end{align}第二步代入了直线方程 要注意这个公式只有 $p_0​$ 可以这样推导出来，因为 $\overline {y_0} = mx_0 + B​$，而其他的 $y_i​$ 就不能这么简单的算出来了。 $\overline{y_i}$ 就是在 $i$ 这个点处，光栅化以后的纵坐标的值 还可以进一步的化简 \begin{aligned} p_{i+1} - p_{i} & = \left( 2 \Delta y \bullet x _ { i + 1 } - 2 \Delta x \cdot \overline { y } _ { i + 1 } + c \right) - \left( 2 \Delta y \bullet x _ { i } - 2 \Delta x \bullet \overline { y } _ { i } + c \right) \\ & = 2 \Delta y - 2 \Delta x \left( \overline { y } _ { i + 1 } - \overline { y } _ { i } \right) \end{aligned} 这里如果 $pi \leq 0$，那么 $\overline {y{i+1}} - \overline{yi} = 0$，那么 $p{i+1} = p_{i} + 2 \Delta y$； 这里如果 $pi &gt; 0$，那么 $\overline {y{i+1}} - \overline{yi} = 1$，那么 $p{i+1} = p_{i} + 2 \Delta y - 2 \Delta x$； 简直是神器啊，得到这个以后就可以开始计算了，全是整数，不浮点不取整不舍入，还是拟合效果最棒的，贴一张图看个例子： Bresenham 直线算法实现 这里使用了几个标志量：flipY和flipXY，表示我们我们对输入数据做的不同的预处理，目的都是把输入的直线转换为一个简单的形式，可以直接应用x=x+1, y=...这样的规则。 其中： 首先判断起始点的x坐标要在终点的x坐标前，如果不符合，则交换两个点的坐标； flipY：应用的情况如下图 目的是将左上到右下的直线转换为左下到右上，就可以使用我们讨论的方法了； flipXY：应用的情况如下图： 此时将纵坐标跨度较长的直线转换为了横坐标跨度较长的直线，也就可以应用我们讨论的办法了； 流程结束后，将得到的直线坐标根据这两个flag进行变换，得到真正的坐标。 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 生成线vector&lt;float&gt; genLinePoints(Point from, Point to) &#123; Points points; bool flipY = false, flipXY = false; int dx = abs(to.x - from.x); int dy = abs(to.y - from.y); if (from.x &gt; to.x) &#123; swap2(from, to); &#125; if (from.y &gt; to.y) &#123; flipY = true; from.y = -from.y; to.y = -to.y; &#125; if (dy &gt; dx) &#123; flipXY = true; swap2(from.x, from.y); swap2(to.x, to.y); &#125; dx = to.x - from.x; dy = to.y - from.y; printf("%d, %d\n", dx, dy); // 意味着要使用 x 作为 +1 更新的坐标 vector&lt;int&gt; p(dx, 2 * dy - dx); points.push_back(from); for (int i = 1; i &lt; dx; i++) &#123; if (p[i - 1] &lt;= 0) &#123; p[i] = p[i - 1] + 2 * dy; &#125; else &#123; p[i] = p[i - 1] + 2 * dy - 2 * dx; &#125; if (p[i] &gt; 0) &#123; points.push_back(Point(from.x + i, points[i - 1].y + 1)); &#125; else &#123; points.push_back(Point(from.x + i, points[i - 1].y)); &#125; &#125; points.push_back(to); if (flipXY) &#123; for (int i = 0; i &lt; points.size(); i++) &#123; swap2(points[i].x, points[i].y); &#125; &#125; if (flipY) &#123; for (int i = 0; i &lt; points.size(); i++) &#123; points[i].y = -points[i].y; &#125; &#125; return pointsToFloat3(points);&#125; Bresenham 算法绘制三角形就是使用 Bresenham 算法画三条线，结果如下： Bresenham 算法画圆算法原理概述画圆和画直线的思路差不多，具体可以看https://en.wikipedia.org/wiki/Midpoint_circle_algorithm 同时要注意只需要画1/8圆就可以了，其他的部分可以对称过去。 起始的 $d = 3 - (2 * r)$，每次 $x = x + 1$，计算 $d$ 而得到 $y$ 坐标的值。 算法流程如下： 代码实现12345678910111213141516171819202122// 生成圆vector&lt;float&gt; genCirclePositions(Point centre, int radius) &#123; vector&lt;Point&gt; pv; int x = 0, y = radius, d = 3 - (2 * radius); auto eightPoints = circle8(centre, x, y); pv.insert(pv.end(), eightPoints.begin(), eightPoints.end()); while (x &lt; y) &#123; if (d &lt; 0) &#123; d = d + 4 * x + 6; &#125; else &#123; d = d + 4 * (x - y) + 10; y--; &#125; x++; auto eightPoints = circle8(centre, x, y); pv.insert(pv.end(), eightPoints.begin(), eightPoints.end()); &#125; auto data = pointsToFloat3(pv); return data;&#125; 实验结果 Edge Equation 三角形填充算法Edge Equation 算法原理概述三角形的填充使用Edge Equation算法，具体实现流程如下： 计算得到一个矩形框，如下图黄色部分，将三角形全部包括进去； 对矩形框内每个点进行计算，判断其是否在三角形的内部，如果在内部则进行渲染，否则不进行渲染； 这一步使用的是简单的直线方程进行计算，首先我们可以得到两点的直线方程为： (y_1 - y_2)x+(x_2 - x_1)y+x_1 y_2 - x_2 y_1 = 0对每个点应用该公式，判断其正负性，则可以判断是否在三角形内部； 具体使用时可能难以判断其正负性那个是在内部，可以使用第三个点辅助判断，与第三个点带入方程的符号相一致的是在三角内部的点，否则是在三角外部的点； 伪代码如下： Edge Equation算法实现1234567891011121314151617181920212223242526272829303132333435std::function&lt;bool(int x, int y)&gt; genLineEquation(Point p1, Point p2, Point p3) &#123; return [=](int x, int y) -&gt; bool &#123; auto lf = [&amp;](int x, int y) &#123; return (p1.y - p2.y) * x + (p2.x - p1.x) * y + (p1.x * p2.y - p2.x * p1.y); &#125;; if (lf(p3.x, p3.y) &gt;= 0) &#123; return lf(x, y) &gt;= 0; &#125; else &#123; return lf(x, y) &lt;= 0; &#125; &#125;;&#125;;// 生成三角形，通过Edge Eqution算法vector&lt;float&gt; genFilledTriPoints(Point p1, Point p2, Point p3) &#123; vector&lt;Point&gt; points; int maxx = max(p1.x, p2.x, p3.x), maxy = max(p1.y, p2.y, p3.y), minx = min(p1.x, p2.x, p3.x), miny = min(p1.y, p2.y, p3.y); auto l1f = genLineEquation(p2, p1, p3); auto l2f = genLineEquation(p3, p2, p1); auto l3f = genLineEquation(p3, p1, p2); for (int i = minx; i &lt; maxx; i++) &#123; for (int j = miny; j &lt; maxy; j++) &#123; bool inside = l1f(i, j) &amp;&amp; l2f(i, j) &amp;&amp; l3f(i, j); if (inside) &#123; points.push_back(Point(i, j)); &#125; &#125; &#125; return pointsToFloat3(points);&#125; 实验结果]]></content>
      <categories>
        <category>CG</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>CG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统分析与设计作业1]]></title>
    <url>%2Fposts%2F%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%BE%E8%AE%A1%E4%BD%9C%E4%B8%9A1-2019-03-12%2F</url>
    <content type="text"><![CDATA[系统分析与设计的第一次作业 系统分析与设计作业 1简答1. 软件工程的定义首先翻译一下来自Wikipedia的定义：软件工程是进行软件开发，操作和维护的一种系统化，规范化，可量化的方法，是工程实践在软件开发上的应用和研究。按照我自己的理解，软件工程就是对于软件开发的每一个步骤（开发，操作，维护，成本……）的一个全方位的考量，是管理一款软件全部生产流程的学科。 2. 解释导致 software crisis 本质原因、表现，述说克服软件危机的方法 本质原因：随着计算机的发展，软件的需求越来越复杂，软件规模变得越来越大，计算能力依照摩尔定律进行提升，因此落后的软件生产方式无法控制满足这样的需求，因此产生了软件危机； 表现：比如软件开发进度难以预测，软件开发成本难以控制，用户对产品功能难以满足，软件产品质量无法保证，软件产品难以维护，软件缺少适当的文档资料等； 客服软件危机的方法：首先要引入先进的管理办法，这就是软件工程要学习的内容； 3. 软件生命周期就是对软件开发的各个过程进行了划分，主要可以划分为以下的几个阶段： 可行性分析与计划，需求分析，设计，编码实现，测试，运行和维护 人们又建立了不同的软件生命周期模型，比如瀑布法，划分了五个过程（Requirement需求；Design设计；Implementation编码实现；Verification验证测试；Maintenance维护），其它的模型有不同的划分； 4. SWEBoK 的 15 个知识域（An Overview of the SWEBOK Guide 请中文翻译其名称与简短说明）2 软件工程实践中的知识领域 2.1 Software Requirements 软件要求涉及软件要求的启发、协商、分析、规范和验证。软件行业普遍承认, 当软件工程项目执行不力时, 这些项目就会非常脆弱。软件需求表达了对软件产品的需求和限制, 这些需求和约束有助于解决一些现实世界的问题。 2.2 Software Design 软件设计设计系统或组件的体系结构、组件、接口和其他特征的过程和结果 (ieee 1991)。 2.3 Software Construction 软件构建指通过详细设计、编码、单元测试、集成测试、调试和验证相结合的方式, 详细创建工作软件。 2.4 Software Testing 软件测试测试是一项通过识别缺陷来评估产品质量和改进产品质量的活动。软件测试涉及针对有限的测试用例集上的预期行为对程序的行为进行动态验证。 2.5 Software Maintenance 软件维护软件维护包括增强现有功能、调整软件以在新的和改进的操作环境中运行以及纠正缺陷。 2.6 Software Configuration Management 软件配置管理软件配置管理 (scm) 是在不同时间点识别系统配置的学科, 目的是系统地控制对配置的更改, 以及维护完整性和在整个软件生命周期中配置的可追溯性。 2.7 Software Engineering Management 软件工程管理软件工程管理包括规划、协调、测量、报告和控制项目或计划, 以确保软件的开发和维护是系统的、有纪律的和量化的。 2.8 Software Engineering Process 软件工程过程软件工程负责软件生命周期过程的定义、实施、评估、测量、管理和改进。 2.9 Software Engineering Models and Methods 软件工程模型与方法软件工程模型和方法 ka 解决了包含多个生命周期阶段的方法。 2.10 Software Quality 软件质量软件质量是一个普遍存在的软件生命周期问题。 2.11 Software Engineering Professional Practice 软件工程专业实践软件工程专业实践涉及软件工程师以专业、负责和合乎道德的方式实践软件工程所必须具备的知识、技能和态度。 3 Knowledge Areas Characterizing the Educational Requirements of Software Engineering 软件工程教育要求知识领域 3.1 Software Engineering Economics 软件工程经济学软件工程经济学负责在业务环境中做出决策, 使技术决策与组织的业务目标保持一致。 3.2 Computing Foundations 计算基础涵盖了提供软件工程实践所需的计算背景的基本主题。所涉及的主题包括问题解决技术、抽象、算法和复杂性、编程基础知识、并行和分布式计算的基础知识、计算机组织、操作系统和网络通信。 3.3 Mathematical Foundations 数学基础为软件工程的实践提供必要的数学背景； 3.4 Engineering Foundations 工程基础涵盖了为软件工程实践提供必要的工程背景的基本主题。 5. 简单解释 CMMI 的五个级别。例如：Level 1 - Initial：无序，自发生产模式。成熟度级别 1: 初始 Initial不可预测和被动。工作已经完成, 但经常被推迟和超出预算。 成熟度级别 2: 托管 Managed在项目级别上管理。项目是有计划的、执行的、测量的和控制的。 成熟度级别 3: 定义 Defined主动, 而不是反应。全组织标准为项目、计划和投资组合提供指导。 成熟度级别 4: 量化管理 Quantitatively Managed测量和控制。本组织以数据为导向, 实现量化的业绩改进目标, 这些目标是可预测的, 并与之保持一致, 以满足内部和外部利益攸关方的需求。 成熟度级别 5: 优化 Optimizing稳定灵活。本组织注重持续改进, 旨在对机遇和变化进行支点和反应。组织的稳定性为敏捷性和创新提供了一个平台。 6. 用自己语言简述 SWEBok 或 CMMI （约200字）SWEBok：是IEEE组织的一个项目，软件工程知识体系指南。在这本指南中，IEEE首次建立了软件工程领域知识体系的基线，他们的工作满足了部分想要促进该领域理论和实践的进展的想法。在这之前，社会上的相关工作受到了历史学科经验的指导，但没有受到一些遇到的问题或者解决方法的帮助。它是对过去几年不断发展和演变的知识体系的一个概略和指南，这个知识体系不是静态的，必须随着软件工程的成熟而发展和演变，是软件工程基础设施中一个有价值的元素。 CMMI：全称是Capability Maturity Model Integration，即能力成熟度模型集成，其目的是帮助软件企业对软件工程过程进行管理和改进，增强开发与改进能力，从而能按时地、不超预算地开发出高质量的软件。其所依据的想法是：只要集中精力持续努力去建立有效的软件工程过程的基础结构，不断进行管理的实践和过程的改进，就可以克服软件开发中的困难。CMMI为改进一个组织的各种过程提供了一个单一的集成化框架，新的集成模型框架消除了各个模型的不一致性，减少了模型间的重复，增加透明度和理解，建立了一个自动的、可扩展的框架。因而能够从总体上改进组织的质量和效率。CMMI主要关注点就是成本效益、明确重点、过程集中和灵活性四个方面。]]></content>
      <categories>
        <category>系统分析与设计</category>
      </categories>
      <tags>
        <tag>系统分析与设计</tag>
        <tag>作业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ImGui入门笔记]]></title>
    <url>%2Fposts%2FImGui%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0-2019-03-10%2F</url>
    <content type="text"><![CDATA[这个玩意资料有点少，所以记录一下怎么开始上路，怎么爬一些坑，供自己和别人参考…… 怎么学这东西这可能是最大的问题了。 首先肯定要看它的官网，https://github.com/ocornut/imgui； 然后你会发现官网的这个RAEDME是个蛇皮没什么大用…… 然后我们发现目录下有个doc目录，进去发现也没什么用，真的是…… 有用的部分在example文件夹下， 比如这个示例！这就是我们学习的最重要的资源了！！！ 安装 首先git clone这个仓库 1git clone https://github.com/ocornut/imgui 然后把目录下的所有.h和.cpp文件都复制到自己的项目里 就这些东西，全部复制进去就对了； 还没完，打开examples文件夹，进去可以看到有好多imgui_impl_***.*，挑选自己需要的那些都复制到自己的项目里；比如我的目录下 错了可能是复制多了几项，来回删一删试试……还是比较迷的…… 然后就可以跑了，跑一下发现报错gl3w.h没有，当然了用的不是这个库当然就没有了……打开imgui_impl_opengl3.h，把这里改成你所使用的库 应该是都在这几个里边了，我使用的是glad，所以使用了glad的宏； 跑跑第一个程序不出意外的话应该是可以跑了，我们可以直接把仓库的examples文件夹里，合适自己的用的那个复制过来试试，比如我用的是glfw+openGL3，我复制了这里的代码（路径是/examples/example_glfw_opengl3/main.cpp），这就可以点点画画了。]]></content>
      <categories>
        <category>ImGui</category>
      </categories>
      <tags>
        <tag>ImGui</tag>
        <tag>入门笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenGL入门笔记]]></title>
    <url>%2Fposts%2FOpenGL%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0-2019-03-09%2F</url>
    <content type="text"><![CDATA[学习OpenGL还是挺难理解的，入门路上遇到了一些问题，在这里埋埋坑，希望可以帮助一些后来者 主要的学习来源是learnopengl的中文教程官网； 基本不会写到具体的过程，具体的过程在教程官网上已经非常详细了，所以这里基本上会写成类似于Q&amp;A，或者记录一些不理解的地方的形式…… 链接顶点属性EBO/IBO，索引这里讲的不错，https://blog.csdn.net/fenghen777/article/details/46981781 我们还是按照官网的教程来讲，官网给了这样一个例子，要画一个4边形； 12345678910float vertices[] = &#123; // 第一个三角形 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, 0.5f, 0.0f, // 左上角 // 第二个三角形 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角&#125;; 为什么是6个点呢，因为OpenGL是按照三角形来渲染的，所以一个四边形就要分为两个三角形来绘制。 这样就造成了一个问题，有两个点是重复的，我们当然不想重复，不优雅啊！（当然最重要的问题是会造成多余的存储开销，多存了50%的点，如果三角形更多的话，这个开销还是很可观的）。 所以我们考虑一个新的方式，就是只输入4个顶点，然后我们指定绘制的顺序，就是这样： 1234567891011float vertices[] = &#123; 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角&#125;;unsigned int indices[] = &#123; // 注意索引从0开始! 0, 1, 3, // 第一个三角形 1, 2, 3 // 第二个三角形&#125;; 不知道你们怎么样，我第一次看是有点懵，仔细看了一下大概是这样的： 明白了吧，这个顶点的顺序就是顶点数组里的顺序。]]></content>
      <categories>
        <category>OpenGL</category>
      </categories>
      <tags>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 87. Scramble String]]></title>
    <url>%2Fposts%2FLeetCode%2087.%20Scramble%20String-2019-03-07%2F</url>
    <content type="text"><![CDATA[题目Given a string s1, we may represent it as a binary tree by partitioning it to two non-empty substrings recursively. Below is one possible representation of s1 = &quot;great&quot;: 1234567 great / \ gr eat / \ / \g r e at / \ a t To scramble the string, we may choose any non-leaf node and swap its two children. For example, if we choose the node &quot;gr&quot; and swap its two children, it produces a scrambled string &quot;rgeat&quot;. 1234567 rgeat / \ rg eat / \ / \r g e at / \ a t We say that &quot;rgeat&quot; is a scrambled string of &quot;great&quot;. Similarly, if we continue to swap the children of nodes &quot;eat&quot; and &quot;at&quot;, it produces a scrambled string &quot;rgtae&quot;. 1234567 rgtae / \ rg tae / \ / \r g ta e / \ t a We say that &quot;rgtae&quot; is a scrambled string of &quot;great&quot;. Given two strings s1 and s2 of the same length, determine if s2 is a scrambled string of s1. Example 1: 12Input: s1 = &quot;great&quot;, s2 = &quot;rgeat&quot;Output: true Example 2: 12Input: s1 = &quot;abcde&quot;, s2 = &quot;caebd&quot;Output: false Solution 283 / 283 test cases passed. Status: Accepted Runtime: 0 ms 123456789101112131415161718192021222324252627282930class Solution &#123;public: bool isScramble(string s1, string s2) &#123; if(s1==s2) return true; int len = s1.length(); int count[26] = &#123;0&#125;; for(int i=0; i&lt;len; i++) &#123; count[s1[i]-'a']++; count[s2[i]-'a']--; &#125; for(int i=0; i&lt;26; i++) &#123; if(count[i]!=0) return false; &#125; for(int i=1; i&lt;=len-1; i++) &#123; if( isScramble(s1.substr(0,i), s2.substr(0,i)) &amp;&amp; isScramble(s1.substr(i), s2.substr(i))) return true; if( isScramble(s1.substr(0,i), s2.substr(len-i)) &amp;&amp; isScramble(s1.substr(i), s2.substr(0,len-i))) return true; &#125; return false; &#125;&#125;; 题解可以分析一下，两个串是 isScramble 的话，那么其中一个串，是通过另一个串进行多次的交换得来的。交换可以交换两个相邻的字符，也可以交换相邻的子串。 类似的我们可以分析，如果两个串满足这个性质，那么我们可以把它们都分为两个长度相同的部分（前i位，记为s11, s21，和后n-i位，记为s12, s22）。 思路很简单，如果两个串是 isScramble 的话，那么肯定是上下相对应，或者交换后相对应（即满足isScramble的)。也就是说，我们可以通过分别验证这两个性质来确认其是否满足条件，只要有一个满足，那么它就是isScramble的。 在这样的思路上我们可以做一些优化，比如将每次递归的结果存起来，使得递归到已经算过的子串的时候可以直接获取答案而不是再次进行计算。 但是在实际使用中发现优化后的代码运行速度并不快，我推测这是因为当前递归程序的重复计算已经被消除了，相同的子串不会进行多次判断，因而完全无需进行记忆性的递归。 分析 时间复杂度： 这是一个递归算法，每次循环n次，最多递归n层，时间复杂度为O(n!)，这是最坏情况]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>String</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 152. Maximum Product Subarray]]></title>
    <url>%2Fposts%2FLeetcode%20152.%20Maximum%20Product%20Subarray-2019-03-07%2F</url>
    <content type="text"><![CDATA[题目Given an integer array nums, find the contiguous subarray within an array (containing at least one number) which has the largest product. Example 1: 123Input: [2,3,-2,4]Output: 6Explanation: [2,3] has the largest product 6. Example 2: 123Input: [-2,0,-1]Output: 0Explanation: The result cannot be 2, because [-2,-1] is not a subarray. 题解这道题的目标是找到最大乘积的子串，还是比较复杂的，看题目就感觉是一道动态规划，我们首先肯定想到的是这样的递推公式： maxProduct[i] = max\{ maxProduct[i-1] * nums[i] , nums[i] \}其中，maxProduct[i]表示以第i位结尾的所有子串的最大乘积，所以要么选择nums[i]乘进去得到更大的值，要么不选择它，而是从他开始重新计算一个子串。 这个解法会遇到一个问题，比如[1, 2, -3, -5]，在面对两个负数的时候，我们的解法遇到-3时就会把它去掉而在-3的地方重新计算。即，maxProduct[i] = nums[i] = 3 ，但是，如果我们保留maxProduct[2] = maxProduct[i-1]*nums[i] = -6，那么我们在计算到-5时就可以得到更大的值30。 考虑这样的情况，我们要改造我们的算法，我们在保留最大的乘积的同时，保留一个最小的负值，期望这个负值可以让最终得到另一个负值时可以得到更大的计算结果。即： 当nums[i]为正值时： postiveMax[i] = max \{ nums[i] , postiveMax[i-1]*nums[i] \} \\ negtiveMin[i] = min \{ nums[i] , negtiveMin[i-1]*nums[i] \}很简单，最小的负值就是当前的最小负值乘上nums[i]或者本身（重新开始），最大的正值也就是当前的最大正值乘上nums[i]或者本身（重新开始）。 当nums[i]为负值时： postiveMax[i] = max\{ nums[i] , negtiveMin[i-1]*nums[i] \} \\ negtiveMin[i] = min\{ nums[i] , postiveMax[i-1]*nums[i] \};此时，最大正值可以通过最大的负值乘以当前的负nums[i]得来，也就解决了上面的问题。 下面我们进行实现： Solution 112345678910111213141516171819class Solution &#123;public: int maxProduct(vector&lt;int&gt;&amp; nums) &#123; int re = nums[0]; vector&lt;int&gt; postiveMax(nums); vector&lt;int&gt; negtiveMin(nums); for (int i = 1; i &lt; nums.size(); i++) &#123; if ( nums[i] &lt; 0 ) &#123; postiveMax[i] = max( nums[i] , negtiveMin[i-1]*nums[i] ); negtiveMin[i] = min( nums[i] , postiveMax[i-1]*nums[i] ); &#125; else &#123; postiveMax[i] = max( nums[i] , postiveMax[i-1]*nums[i] ); negtiveMin[i] = min( nums[i] , negtiveMin[i-1]*nums[i] ); &#125; re = max(re, postiveMax[i]); &#125; return re; &#125; &#125;; 还可以进行一些小小的优化使得代码量更加减少： 123456789101112131415class Solution &#123;public: int maxProduct(vector&lt;int&gt;&amp; nums) &#123; int re = nums[0]; vector&lt;int&gt; postiveMax(nums), negtiveMin(nums); for (int i = 1; i &lt; nums.size(); i++) &#123; postiveMax[i] = max( postiveMax[i-1]*nums[i] , max( nums[i] , negtiveMin[i-1]*nums[i] ) ); negtiveMin[i] = min( negtiveMin[i-1]*nums[i] , min( nums[i] , postiveMax[i-1]*nums[i] ) ); re = max(re, postiveMax[i]); &#125; return re; &#125; &#125;; 把判断nums[i]正负性的一步去掉了，因为我们其实无需关心到底是取了哪一个，我们只要它拿到最小和最大即可了。 分析我们分析这里的复杂度，发现空间复杂度是O(n)，时间复杂度也是O(n)。 这个解法还是可以更加优化一下的，空间复杂度可以降为O(1)，即每次只保留前一次(max[i-1])的结果就好了， 不需要知道前(max[i-2, i-3, ...])的内容。]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Algorithm</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 264. Ugly Number II]]></title>
    <url>%2Fposts%2FLeetcode%20264.%20Ugly%20Number%20II-2019-03-07%2F</url>
    <content type="text"><![CDATA[Write a program to find the n-th ugly number. Ugly numbers are positive numbers whose prime factors only include 2, 3, 5. Example: 123Input: n = 10Output: 12Explanation: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12 is the sequence of the first 10 ugly numbers. Note: 1 is typically treated as an ugly number. n does not exceed 1690. Solution123456789101112131415class Solution &#123;public: int nthUglyNumber(int n) &#123; int i2 = 0, i3 = 0, i5 = 0; vector&lt;int&gt; dp &#123;1&#125;; for (int i = 0; i &lt; n; i++) &#123; int m = min( min(dp[i2]*2, dp[i3]*3), dp[i5]*5 ); dp.push_back(m); if (m == dp[i2]*2) i2++; if (m == dp[i3]*3) i3++; if (m == dp[i5]*5) i5++; &#125; return dp[n-1]; &#125;&#125;; 题解应该大家看着还是挺迷茫的，我们从题目开始思考，要找的是质因数只有2、3、5的数，那么我们可以产生两种思路，一种是验证的思路，对于每个数进行验证，判断他是否有2、3、5之外的质因子；或者是采用构造的思路，从某个数开始构造满足这个条件的数字。 明显第一种思路耗时很久，毕竟现在还是没有很好地判断因子分解的算法。从另一种思路出发，我们可以这样考虑：所有以2、3、5为因子的数相乘并不会产生其他的素数作为因子，我们可以简单地推导一下 很明显素数 $p$ 和 $q$ 相乘时，$pq$不存在其他的素数作为它的因子； $ p^i q^j $明显也没有其他素数作为因子 因此这个算法应该是成立的！ 因此，又为了排序的目的，我们逐个取最小值产生ugly number的数组。 如下： 1234567t2 = 0, t3 = 0, t5 = 0 uglg numbers [0] = 1for i from 1 to n uglg numbers [i] = min( uglg numbers [t2], uglg numbers [t3], uglg numbers [t5])return uglg numbers [n-1]]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 97. Interleaving String]]></title>
    <url>%2Fposts%2FLeetcode%2097.%20Interleaving%20String-2019-03-07%2F</url>
    <content type="text"><![CDATA[Given s1, s2, s3, find whether s3 is formed by the interleaving of s1 and s2. Example 1: 12Input: s1 = &quot;aabcc&quot;, s2 = &quot;dbbca&quot;, s3 = &quot;aadbbcbcac&quot;Output: true Example 2: 12Input: s1 = &quot;aabcc&quot;, s2 = &quot;dbbca&quot;, s3 = &quot;aadbbbaccc&quot;Output: false 题解使用动态规划求解。 首先我们分解子问题。如果s3是s1和s2的交错字符串的话，那么一定有以下的一种情况 s3的最后一位和s2的最后一位相同，此时s3前边的子串(除去最后一位)一定是s2的前边的子串与s1的交错字符串。 s3的最后一位和s1的最后一位相同，此时s3前边的子串(除去最后一位)一定是s1的前边的子串与s2的交错字符串。 这个很容易想到，因为构造出交错的字符串的时候肯定是从s1选1个char，又从s2选1个char，所以如果当前是交错的串的话，去掉最后一位相同的位之后，前边的串肯定也是根据这个规则构造来的，所以也是交错的字符串。 因此我们可以写出迭代方程： length·of·s3 = l \\ length·of·s2 = n \\ length·of·s1 = m \\ isInterleave(s3[0...l]) = \\ (s3[l] == s2[n] ^ isInterleave(s1[0...m], s2[0...n-1], s3[0...l-1])) \\ or (s3[l] == s1[m] ^ isInterleave(s1[0...m-1], s2[0...n], s3[0...l-1])) \\基于此方程我们写出如下的解： Solution12345678910111213141516171819202122232425class Solution &#123;public: bool isInterleave(string s1, string s2, string s3) &#123; int m = s1.length(), n = s2.length(), l = s3.length(); if (m + n != l) return false; vector&lt;vector&lt;bool&gt;&gt; dp(m+1, vector&lt;bool&gt;(n+1, false)); for (int i = 0; i &lt; m+1; i++) &#123; for (int j = 0; j &lt; n+1; j++) &#123; if (i == 0 &amp;&amp; j == 0) dp[0][0] = true; else if (i == 0) dp[i][j] = dp[i][j-1] &amp;&amp; s2[j-1] == s3[i+j-1]; // when i = 0, j = 1 // dp[0][1] = dp[0][0] and s2[1] == s3[1] else if (j == 0) dp[i][j] = dp[i-1][j] &amp;&amp; s1[i-1] == s3[i+j-1]; // when i = 1, j = 0 // dp[1][0] = dp[0][0] and s1[1] == s3[1] else dp[i][j] = (dp[i-1][j] &amp;&amp; s1[i-1]==s3[i+j-1]) || (dp[i][j-1] &amp;&amp; s2[j-1]==s3[i+j-1]); // when i = 1, j = 1 // dp[1][1] = // | dp[0][1] &amp;&amp; s1[1] == s3[2] // | dp[1][0] &amp;&amp; s2[1] == s3[1] &#125; &#125; return dp[m][n]; &#125;&#125;;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>String</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fposts%2Fhello-world-2018-09-21%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. David LevithanWide Awake]]></content>
  </entry>
</search>
